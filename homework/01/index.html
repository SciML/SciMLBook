<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/tufte.css"> <link rel=stylesheet  href="/css/latex.css"> <link rel=stylesheet  href="/css/adjust.css"> <link rel=icon  href="/assets/favicon.png"> <link rel=stylesheet  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <link rel=stylesheet  href="/css/weave.css"> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, TeX: { equationNumbers: { autoNumber: "AMS" } } }); </script> <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> <title>Homework 1 - MIT Parallel Computing and Scientific Machine Learning (SciML)</title> <div id=layout > <div id=menu > <ul> <li><a style="font-size:larger;" href=https://github.com/SciML/SciMLBook><i class="fa fa-github"></i></a> <li><a style="font-size:larger;" href="/">Home</a> <li><a style="font-size:larger;" href="/course/">Course</a> <li><a style="font-size:larger;" href="/homework/">Homework</a> <li><a style="font-size:larger;" href="/lectures/">Lectures</a> <li><a style="font-size:larger;" href="/notes/">Notes</a> <ul style="font-size:smaller"> <li><a href="/notes/02-Optimizing_Serial_Code/">02: Serial Code</a> <li><a href="/notes/03-Introduction_to_Scientific_Machine_Learning_through_Physics-Informed_Neural_Networks/">03: SciML Intro</a> <li><a href="/notes/04-How_Loops_Work-An_Introduction_to_Discrete_Dynamics/">04: How Loops Work</a> <li><a href="/notes/05-The_Basics_of_Single_Node_Parallel_Computing/">05: Basics of Parallelism</a> <li><a href="/notes/06-The_Different_Flavors_of_Parallelism/">06: Flavors of Parallelism</a> <li><a href="/notes/07-Ordinary_Differential_Equations-Applications_and_Discretizations/">07: ODEs</a> <li><a href="/notes/08-Forward-Mode_Automatic_Differentiation_(AD)_via_High_Dimensional_Algebras/">08: Forward AD</a> <li><a href="/notes/09-Solving_Stiff_Ordinary_Differential_Equations/">09: Stiff ODEs</a> <li><a href="/notes/10-Basic_Parameter_Estimation-Reverse-Mode_AD-and_Inverse_Problems/">10: Reverse AD</a> <li><a href="/notes/11-Differentiable_Programming_and_Neural_Differential_Equations/">11: Î´P</a> <li><a href="/notes/12-Description_of_MPI_and_MPI/">12: MPI</a> <li><a href="/notes/13-GPU_programming/">13: GPUs</a> <li><a href="/notes/14-PDEs_Convolutions_and_the_Mathematics_of_Locality/">14: PDEs</a> <li><a href="/notes/15-Mixing_Differential_Equations_and_Neural_Networks_for_Physics-Informed_Learning/">15: Physics Informed Learning</a> <li><a href="/notes/16-From_Optimization_to_Probabilistic_Programming/">16: Probabilistic Programming</a> <li><a href="/notes/17-Global_Sensitivity_Analysis/">17: Global Sensitivity Analysis</a> <li><a href="/notes/18-Code_Profiling_and_Optimization/">18: Profiling & Optimization</a> <li><a href="/notes/19-Uncertainty_Programming-Generalized_Uncertainty_Quantification/">19: Uncertainty Programming</a> </ul> </ul> </div> <div id=main > <div class=franklin-content > <h1 class=title >Homework 1, Parallelized Dynamics</h1> <h5>Chris Rackauckas</h5> <h5>September 15th, 2020</h5> <p>Due October 1st, 2020 at midnight EST.</p> <p>Homework 1 is a chance to get some experience implementing discrete dynamical systems techniques in a way that is parallelized, and a time to understand the fundamental behavior of the bottleneck algorithms in scientific computing.</p> <p>Please email the results to <code>crackauc@mit.edu</code> titled &quot;<strong>Homework 1: &#40;Your Name&#41;</strong>&quot;.</p> <h2>Problem 1: A Ton of New Facts on Newton</h2> <p>In this problem we will look into Newton&#39;s method. Newton&#39;s method is the dynamical system defined by the update process:</p> <p class=math >\[ x_{n+1} = x_n - \left(\frac{dg}{dx}(x_n)\right)^{-1} g(x_n) \]</p> <p>For these problems, assume that <span class=math >$\frac{dg}{dx}$</span> is non-singular.</p> <h3>Part 1</h3> <p>Show that if <span class=math >$x^\ast$</span> is a steady state of the equation, then <span class=math >$g(x^\ast) = 0$</span>.</p> <h3>Part 2</h3> <p>Take a look at the Quasi-Newton approximation:</p> <p class=math >\[ x_{n+1} = x_n - \left(\frac{dg}{dx}(x_0)\right)^{-1} g(x_n) \]</p> <p>for some fixed <span class=math >$x_0$</span>. Derive the stability of the Quasi-Newton approximation in the form of a matrix whose eigenvalues need to be constrained. Use this to argue that if <span class=math >$x_0$</span> is sufficiently close to <span class=math >$x^\ast$</span> then the steady state is a stable &#40;attracting&#41; steady state.</p> <h3>Part 3</h3> <p>Relaxed Quasi-Newton is the method:</p> <p class=math >\[ x_{n+1} = x_n - \alpha \left(\frac{dg}{dx}(x_0)\right)^{-1} g(x_n) \]</p> <p>Argue that for some sufficiently small <span class=math >$\alpha$</span> that the Quasi-Newton iterations will be stable if the eigenvalues of <span class=math >$(\left(\frac{dg}{dx}(x_0)\right)^{-1} g(x_n))^\prime$</span> are all positive for every <span class=math >$x$</span>.</p> <p>&#40;Technically, these assumptions can be greatly relaxed, but weird cases arise. When <span class=math >$x \in \mathbb{C}$</span>, this holds except on some set of Lebesgue measure zero. Feel free to explore this.&#41;</p> <h3>Part 4</h3> <p>Fixed point iteration is the dynamical system</p> <p class=math >\[ x_{n+1} = g(x_n) \]</p> <p>which converges to <span class=math >$g(x)=x$</span>.</p> <ol> <li><p>What is a small change to the dynamical system that could be done such that <span class=math >$g(x)=0$</span> is the steady state?</p> <li><p>How can you change the <span class=math >$\left(\frac{dg}{dx}(x_0)\right)^{-1}$</span> term from the Quasi-Newton iteration to get a method equivalent to fixed point iteration? What does this imply about the difference in stability between Quasi-Newton and fixed point iteration if <span class=math >$\frac{dg}{dx}$</span> has large eigenvalues?</p> </ol> <h2>Problem 2: The Root of all Problems</h2> <p>In this problem we will practice writing fast and type-generic Julia code by producing an algorithm that will compute the quantile of any probability distribution.</p> <h3>Part 1</h3> <p>Many problems can be interpreted as a rootfinding problem. For example, let&#39;s take a look at a problem in statistics. Let <span class=math >$X$</span> be a random variable with a cumulative distribution function &#40;CDF&#41; of <span class=math >$cdf(x)$</span>. Recall that the CDF is a monotonically increasing function in <span class=math >$[0,1]$</span> which is the total probability of <span class=math >$X < x$</span>. The <span class=math >$y$</span>th quantile of <span class=math >$X$</span> is the value <span class=math >$x$</span> at with <span class=math >$X$</span> has a y&#37; chance of being less than <span class=math >$x$</span>. Interpret the problem of computing an arbitrary quantile <span class=math >$y$</span> as a rootfinding problem, and use Newton&#39;s method to write an algorithm for computing <span class=math >$x$</span>.</p> <p>&#40;Hint: Recall that <span class=math >$cdf^{\prime}(x) = pdf(x)$</span>, the probability distribution function.&#41;</p> <h3>Part 2</h3> <p>Use the types from Distributions.jl to write a function <code>my_quantile&#40;y,d&#41;</code> which uses multiple dispatch to compute the <span class=math >$y$</span>th quantile for any <code>UnivariateDistribution</code> <code>d</code> from Distributions.jl. Test your function on <code>Gamma&#40;5, 1&#41;</code>, <code>Normal&#40;0, 1&#41;</code>, and <code>Beta&#40;2, 4&#41;</code> against the <code>Distributions.quantile</code> function built into the library.</p> <p>&#40;Hint: Have a keyword argument for <span class=math >$x_0$</span>, and let its default be the mean or median of the distribution.&#41;</p> <h2>Problem 3: Bifurcating Data for Parallelism</h2> <p>In this problem we will write code for efficient generation of the bifurcation diagram of the logistic equation.</p> <h3>Part 1</h3> <p>The logistic equation is the dynamical system given by the update relation:</p> <p class=math >\[ x_{n+1} = rx_n (1-x_n) \]</p> <p>where <span class=math >$r$</span> is some parameter. Write a function which iterates the equation from <span class=math >$x_0 = 0.25$</span> enough times to be sufficiently close to its long-term behavior &#40;400 iterations&#41; and samples 150 points from the steady state attractor &#40;i.e. output iterations 401:550&#41; as a function of <span class=math >$r$</span>, and mutates some vector as a solution, i.e. <code>calc_attractor&#33;&#40;out,f,p,num_attract&#61;150;warmup&#61;400&#41;</code>.</p> <p>Test your function with <span class=math >$r = 2.9$</span>. Double check that your function computes the correct result by calculating the analytical steady state value.</p> <h3>Part 2</h3> <p>The bifurcation plot shows how a steady state changes as a parameter changes. Compute the long-term result of the logistic equation at the values of <code>r &#61; 2.9:0.001:4</code>, and plot the steady state values for each <span class=math >$r$</span> as an r x steady_attractor scatter plot. You should get a very bizarrely awesome picture, the bifurcation graph of the logistic equation.</p> <p><img src="https://upload.wikimedia.org/wikipedia/commons/7/7d/LogisticMap_BifurcationDiagram.png" alt="" /></p> <p>&#40;Hint: Generate a single matrix for the attractor values, and use <code>calc_attractor&#33;</code> on views of columns for calculating the output, or inline the <code>calc_attractor&#33;</code> computation directly onto the matrix, or even give <code>calc_attractor&#33;</code> an input for what column to modify.&#41;</p> <h3>Part 3</h3> <p>Multithread your bifurcation graph generator by performing different steady state calcuations on different threads. Does your timing improve? Why? Be careful and check to make sure you have more than 1 thread&#33;</p> <h3>Part 4</h3> <p>Multiprocess your bifurcation graph generator first by using <code>pmap</code>, and then by using <code>@distributed</code>. Does your timing improve? Why? Be careful to add processes before doing the distributed call.</p> <p>&#40;Note: You may need to change your implementation around to be allocating differently in order for it to be compatible with multiprocessing&#33;&#41;</p> <h3>Part 5</h3> <p>Which method is the fastest? Why?</p> <div class=footer > <p> Published from <a href=hw1.jmd >hw1.jmd</a> using <a href="http://github.com/JunoLab/Weave.jl">Weave.jl</a> v0.10.9 on 2023-01-13. </p> </div> <div class=back-to-top > <span><a href="#" title="Back to Top"><i class="fa fa-chevron-circle-up"></i></a></span> </div> </div> </div> </div>