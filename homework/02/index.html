<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/tufte.css"> <link rel=stylesheet  href="/css/latex.css"> <link rel=stylesheet  href="/css/adjust.css"> <link rel=icon  href="/assets/favicon.png"> <link rel=stylesheet  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <link rel=stylesheet  href="/css/weave.css"> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, TeX: { equationNumbers: { autoNumber: "AMS" } } }); </script> <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> <title>Homework 2 - MIT Parallel Computing and Scientific Machine Learning (SciML)</title> <div id=layout > <div id=menu > <ul> <li><a style="font-size:larger;" href=https://github.com/SciML/SciMLBook><i class="fa fa-github"></i></a> <li><a style="font-size:larger;" href="/">Home</a> <li><a style="font-size:larger;" href="/course/">Course</a> <li><a style="font-size:larger;" href="/homework/">Homework</a> <li><a style="font-size:larger;" href="/lectures/">Lectures</a> <li><a style="font-size:larger;" href="/notes/">Notes</a> <ul style="font-size:smaller"> <li><a href="/notes/02-Optimizing_Serial_Code/">02: Serial Code</a> <li><a href="/notes/03-Introduction_to_Scientific_Machine_Learning_through_Physics-Informed_Neural_Networks/">03: SciML Intro</a> <li><a href="/notes/04-How_Loops_Work-An_Introduction_to_Discrete_Dynamics/">04: How Loops Work</a> <li><a href="/notes/05-The_Basics_of_Single_Node_Parallel_Computing/">05: Basics of Parallelism</a> <li><a href="/notes/06-The_Different_Flavors_of_Parallelism/">06: Flavors of Parallelism</a> <li><a href="/notes/07-Ordinary_Differential_Equations-Applications_and_Discretizations/">07: ODEs</a> <li><a href="/notes/08-Forward-Mode_Automatic_Differentiation_(AD)_via_High_Dimensional_Algebras/">08: Forward AD</a> <li><a href="/notes/09-Solving_Stiff_Ordinary_Differential_Equations/">09: Stiff ODEs</a> <li><a href="/notes/10-Basic_Parameter_Estimation-Reverse-Mode_AD-and_Inverse_Problems/">10: Reverse AD</a> <li><a href="/notes/11-Differentiable_Programming_and_Neural_Differential_Equations/">11: Î´P</a> <li><a href="/notes/12-Description_of_MPI_and_MPI/">12: MPI</a> <li><a href="/notes/13-GPU_programming/">13: GPUs</a> <li><a href="/notes/14-PDEs_Convolutions_and_the_Mathematics_of_Locality/">14: PDEs</a> <li><a href="/notes/15-Mixing_Differential_Equations_and_Neural_Networks_for_Physics-Informed_Learning/">15: Physics Informed Learning</a> <li><a href="/notes/16-From_Optimization_to_Probabilistic_Programming/">16: Probabilistic Programming</a> <li><a href="/notes/17-Global_Sensitivity_Analysis/">17: Global Sensitivity Analysis</a> <li><a href="/notes/18-Code_Profiling_and_Optimization/">18: Profiling & Optimization</a> <li><a href="/notes/19-Uncertainty_Programming-Generalized_Uncertainty_Quantification/">19: Uncertainty Programming</a> </ul> </ul> </div> <div id=main > <div class=franklin-content > <h1 class=title >Homework 2</h1> <h5>Chris Rackauckas</h5> <h5>October 8th, 2020</h5> <p>Due November 5th, 2020 at midnight.</p> <p>Please email the results to <code>18337.mit.psets@gmail.com</code>.</p> <h2>Problem 1: Parameter Estimation in Dynamical Systems</h2> <p>In this problem you will work through the development of a parameter estimation method for the Lotka-Volterra equations. This is intended to be &quot;from scratch&quot;, meaning you should not use pre-built differential equation solvers for these problems&#33;</p> <h3>Part 1: Simulator</h3> <p>Implement a fixed timestep version of the <a href="https://en.wikipedia.org/wiki/Dormand&#37;E2&#37;80&#37;93Prince_method">Dormand-Prince method</a> to solve the Lotka-Volterra equations:</p> <p class=math >\[ \begin{align} \frac{dx}{dt} &= \alpha x - \beta x y\\ \frac{dy}{dt} &= - \gamma y + \delta x y\\ \end{align} \]</p> <p>with <span class=math >$u(0) = [x(0),y(0)] = [1,1]$</span>, <span class=math >$dt=\frac{1}{4}$</span> on <span class=math >$t \in (0,10)$</span> with parameters <span class=math >$\alpha = 1.5$</span>, <span class=math >$\beta = 1.0$</span>, <span class=math >$\gamma = 3.0$</span>, and <span class=math >$\delta = 1.0$</span>.</p> <h3>Part 2: Forward Sensitivity Equations</h3> <p>The forward sensitivity equations are derived directly by the chain rule. The forward sensitivity equations are equivalent to the result of forward-mode automatic differentiation on the ODE solver. In this part we will implement the sensitivity equations by hand. Given an ODE:</p> <p class=math >\[ u' = f(u,p,t) \]</p> <p>The forward sensitivity equations are given by:</p> <p class=math >\[ \frac{d}{dt}\left(\frac{du}{dp}\right) = \frac{df}{du}\frac{du}{dp} + \frac{df}{dp} \]</p> <p>Use this definition to simultaneously solve for the solution to the ODE along with its derivative with respect to parameters.</p> <h3>Part 3: Parameter Estimation</h3> <p>Generate data using the parameters from Part 1. Then perturb the parameters to start at <span class=math >$\alpha = 1.2$</span>, <span class=math >$\beta = 0.8$</span>, <span class=math >$\gamma = 2.8$</span>, and <span class=math >$\delta = 0.8$</span>. Use the L2 loss against the data as a cost function, and use the forward sensitivity equations to implement gradient descent and optimize the cost function to retrieve the true parameters</p> <h2>Problem 2: Bandwidth Maximization</h2> <p>In this problem we will look at maximizing the bandwidth of a channel, and the difference when going from serial to parallel computing.</p> <h3>Part 1: Serial Bandwidth</h3> <p>Recreate the bandwidth vs message size plot by doing the following. Create two processes on one node and communicate using MPI. Have both processes send an array of <code>2^n</code> <code>Int8</code> values. Compute the time <code>T&#40;n&#41;</code> of sending each message size and the bandwidth <code>B&#40;n&#41; &#61; n/T&#40;n&#41;</code> for <code>n&#61;1:25</code>.</p> <h3>Part 2: Interpretation</h3> <p>Given these plots, what is the minimal latency? What is the total bandwidth? What is the number of operations needed for small messages and large messages on each value in order for a calculation to be efficient enough to overcome the message passing cost?</p> <h3>Part 3: Parallel Bandwith &#40;Optional&#41;</h3> <p>Use the following batch script on some cluster &#40;like MIT Supercloud&#41;:</p> <pre><code class="language-bash;eval&#61;false">#&#33;/bin/sh

#SBATCH -o mylog.out-&#37;j
#SBATCH -n 2
#SBATCH -N 2

# Initialize Modules
source /etc/profile

# Load Julia and MPI Modules
module load julia-latest
module load mpi/mpich-x86_64

mpirun julia mycode.jl</code></pre> <p>to receive two cores on two nodes. Recreate the bandwidth vs message plots and the interpretation. Does the fact that the nodes are physically disconnected cause a substantial difference?</p> <div class=footer > <p> Published from <a href=hw2.jmd >hw2.jmd</a> using <a href="http://github.com/JunoLab/Weave.jl">Weave.jl</a> v0.10.9 on 2023-02-28. </p> </div> <div class=back-to-top > <span><a href="#" title="Back to Top"><i class="fa fa-chevron-circle-up"></i></a></span> </div> </div> </div> </div>