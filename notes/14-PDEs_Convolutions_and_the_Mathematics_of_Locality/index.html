<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/tufte.css"> <link rel=stylesheet  href="/css/latex.css"> <link rel=stylesheet  href="/css/adjust.css"> <link rel=icon  href="/assets/favicon.png"> <link rel=stylesheet  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <link rel=stylesheet  href="/css/weave.css"> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, TeX: { equationNumbers: { autoNumber: "AMS" } } }); </script> <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> <title>PDEs, Convolutions, and the Mathematics of Locality - MIT Parallel Computing and Scientific Machine Learning (SciML)</title> <div id=layout > <div id=menu > <ul> <li><a style="font-size:larger;" href=https://github.com/SciML/SciMLBook><i class="fa fa-github"></i></a> <li><a style="font-size:larger;" href="/">Home</a> <li><a style="font-size:larger;" href="/course/">Course</a> <li><a style="font-size:larger;" href="/homework/">Homework</a> <li><a style="font-size:larger;" href="/lectures/">Lectures</a> <li><a style="font-size:larger;" href="/notes/">Notes</a> <ul style="font-size:smaller"> <li><a href="/notes/02-Optimizing_Serial_Code/">02: Serial Code</a> <li><a href="/notes/03-Introduction_to_Scientific_Machine_Learning_through_Physics-Informed_Neural_Networks/">03: SciML Intro</a> <li><a href="/notes/04-How_Loops_Work-An_Introduction_to_Discrete_Dynamics/">04: How Loops Work</a> <li><a href="/notes/05-The_Basics_of_Single_Node_Parallel_Computing/">05: Basics of Parallelism</a> <li><a href="/notes/06-The_Different_Flavors_of_Parallelism/">06: Flavors of Parallelism</a> <li><a href="/notes/07-Ordinary_Differential_Equations-Applications_and_Discretizations/">07: ODEs</a> <li><a href="/notes/08-Forward-Mode_Automatic_Differentiation_(AD)_via_High_Dimensional_Algebras/">08: Forward AD</a> <li><a href="/notes/09-Solving_Stiff_Ordinary_Differential_Equations/">09: Stiff ODEs</a> <li><a href="/notes/10-Basic_Parameter_Estimation-Reverse-Mode_AD-and_Inverse_Problems/">10: Reverse AD</a> <li><a href="/notes/11-Differentiable_Programming_and_Neural_Differential_Equations/">11: δP</a> <li><a href="/notes/12-Description_of_MPI_and_MPI/">12: MPI</a> <li><a href="/notes/13-GPU_programming/">13: GPUs</a> <li><a href="/notes/14-PDEs_Convolutions_and_the_Mathematics_of_Locality/">14: PDEs</a> <li><a href="/notes/15-Mixing_Differential_Equations_and_Neural_Networks_for_Physics-Informed_Learning/">15: Physics Informed Learning</a> <li><a href="/notes/16-From_Optimization_to_Probabilistic_Programming/">16: Probabilistic Programming</a> <li><a href="/notes/17-Global_Sensitivity_Analysis/">17: Global Sensitivity Analysis</a> <li><a href="/notes/18-Code_Profiling_and_Optimization/">18: Profiling & Optimization</a> <li><a href="/notes/19-Uncertainty_Programming-Generalized_Uncertainty_Quantification/">19: Uncertainty Programming</a> </ul> </ul> </div> <div id=main > <div class=franklin-content > <h1 class=title >PDEs, Convolutions, and the Mathematics of Locality</h1> <h5>Chris Rackauckas</h5> <h5>December 4th, 2020</h5> <h2><a href="https://youtu.be/apkyk8n0vBo">Youtube Video</a></h2> <p>At this point we have identified how the worlds of machine learning and scientific computing collide by looking at the parameter estimation problem. Training neural networks is parameter estimation of a function <code>f</code> where <code>f</code> is a neural network. Backpropagation of a neural network is simply the adjoint problem for <code>f</code>, and it falls under the class of methods used in reverse-mode automatic differentiation. But this story also extends to structure. Recurrent neural networks are the Euler discretization of a continuous recurrent neural network, also known as a neural ordinary differential equation.</p> <p>Given all of these relations, our next focus will be on the other class of commonly used neural networks: the <em>convolutional neural network</em> &#40;CNN&#41;. It turns out that in this case there is also a clear analogue to convolutional neural networks in traditional scientific computing, and this is seen in discretizations of partial differential equations. To see this, we will first describe the convolution operation that is central to the CNN and see how this object naturally arises in numerical partial differential equations.</p> <h2>Convolutional Neural Networks</h2> <p>The purpose of a convolutional neural network is to be a network which makes use of the spatial structure of an image. An image is a 3-dimensional object: width, height, and 3 color channels. The convolutional operations keeps this structure intact and acts against this object is a 3-tensor. A convolutional layer is a function that applies a <em>stencil</em> to each point. This is illustrated by the following animation:</p> <p><img src="https://miro.medium.com/max/526/1*GcI7G-JLAQiEoCON7xFbhg.gif" alt=convolution  /></p> <p>This is the 2D stencil:</p> <pre><code>1  0  1
0  1  0
1  0  1</code></pre> <p>which is then applied to the matrix at each inner point to go from an NxNx3 matrix to an &#40;N-2&#41;x&#40;N-2&#41;x3 matrix.</p> <p>Another operation used with convolutions is the <em>pooling layer</em>. For example, the <em>maxpool</em> layer is stencil which takes the maximum of the the value and its neighbor, and the <em>meanpool</em> takes the mean over the nearby values, i.e. it is equivalent to the stencil:</p> <pre><code>1/9 1/9 1/9
1/9 1/9 1/9
1/9 1/9 1/9</code></pre> <p>A convolutional neural network is then composed of layers of this form. We can express this mathematically by letting <span class=math >$conv(x;S)$</span> as the convolution of <span class=math >$x$</span> given a stencil <span class=math >$S$</span>. If we let <span class=math >$dense(x;W,b,σ) = σ(W*x + b)$</span> as a layer from a standard neural network, then deep convolutional neural networks are of forms like:</p> <p class=math >\[ CNN(x) = dense(conv(maxpool(conv(x)))) \]</p> <p>which can be expressed in Flux.jl syntax as:</p> <pre class='hljl'>
<span class='hljl-n'>m</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>Chain</span><span class='hljl-p'>(</span><span class='hljl-t'>
  </span><span class='hljl-nf'>Conv</span><span class='hljl-p'>((</span><span class='hljl-ni'>2</span><span class='hljl-p'>,</span><span class='hljl-ni'>2</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>=&gt;</span><span class='hljl-ni'>16</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>relu</span><span class='hljl-p'>),</span><span class='hljl-t'>
  </span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>-&gt;</span><span class='hljl-t'> </span><span class='hljl-nf'>maxpool</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-ni'>2</span><span class='hljl-p'>,</span><span class='hljl-ni'>2</span><span class='hljl-p'>)),</span><span class='hljl-t'>
  </span><span class='hljl-nf'>Conv</span><span class='hljl-p'>((</span><span class='hljl-ni'>2</span><span class='hljl-p'>,</span><span class='hljl-ni'>2</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-ni'>16</span><span class='hljl-oB'>=&gt;</span><span class='hljl-ni'>8</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>relu</span><span class='hljl-p'>),</span><span class='hljl-t'>
  </span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>-&gt;</span><span class='hljl-t'> </span><span class='hljl-nf'>maxpool</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-ni'>2</span><span class='hljl-p'>,</span><span class='hljl-ni'>2</span><span class='hljl-p'>)),</span><span class='hljl-t'>
  </span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>-&gt;</span><span class='hljl-t'> </span><span class='hljl-nf'>reshape</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>:</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>4</span><span class='hljl-p'>)),</span><span class='hljl-t'>
  </span><span class='hljl-nf'>Dense</span><span class='hljl-p'>(</span><span class='hljl-ni'>288</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>10</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-n'>softmax</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>|&gt;</span><span class='hljl-t'> </span><span class='hljl-n'>gpu</span>
</pre> <h2>Discretizations of Partial Differential Equations</h2> <p>Now let&#39;s investigate discretizations of partial differential equations. A canonical differential equation to start with is the Poisson equation. This is the equation:</p> <p class=math >\[ u_{xx} = f(x) \]</p> <p>where here we have that subscripts correspond to partial derivatives, i.e. this syntax stands for the partial differential equation:</p> <p class=math >\[ \frac{d^2u}{dx^2} = f(x) \]</p> <p>In this case, <span class=math >$f$</span> is some given data and the goal is to find the <span class=math >$u$</span> that satisfies this equation. There are two ways this is generally done:</p> <ol> <li><p>Expand out the derivative in terms of Taylor series approximations.</p> <li><p>Expand out <span class=math >$u$</span> in terms of some function basis.</p> </ol> <h3>Finite Difference Discretizations</h3> <p>Let&#39;s start by looking at Taylor series approximations to the derivative. In this case, we will use what&#39;s known as finite differences. The simplest finite difference approximation is known as the first order forward difference. This is commonly denoted as</p> <p class=math >\[ \delta_{+}u=\frac{u(x+\Delta x)-u(x)}{\Delta x} \]</p> <p>This looks like a derivative, and we think it&#39;s a derivative as <span class=math >$\Delta x\rightarrow 0$</span>, but let&#39;s show that this approximation is meaningful. Assume that <span class=math >$u$</span> is sufficiently nice. Then from a Taylor series we have that</p> <p class=math >\[ u(x+\Delta x)=u(x)+\Delta xu^{\prime}(x)+\mathcal{O}(\Delta x^{2}) \]</p> <p>&#40;here I write <span class=math >$\left(\Delta x\right)^{2}$</span> as <span class=math >$\Delta x^{2}$</span> out of convenience, note that those two terms are not necessarily the same&#41;. That term on the end is called “Big-O Notation”. What is means is that those terms are asymptotically like <span class=math >$\Delta x^{2}$</span>. If <span class=math >$\Delta x$</span> is small, then <span class=math >$\Delta x^{2}\ll\Delta x$</span> and so we can think of those terms as smaller than any of the terms we show in the expansion. By simplification notice that we get</p> <p class=math >\[ \frac{u(x+\Delta x)-u(x)}{\Delta x}=u^{\prime}(x)+\mathcal{O}(\Delta x) \]</p> <p>This means that <span class=math >$\delta_{+}$</span> is correct up to first order, where the <span class=math >$\mathcal{O}(\Delta x)$</span> portion that we dropped is the error. Thus <span class=math >$\delta_{+}$</span> is a first order approximation.</p> <p>Notice that the same proof shows that the backwards difference,</p> <p class=math >\[ \delta_{-}u=\frac{u(x)-u(x-\Delta x)}{\Delta x} \]</p> <p>is first order.</p> <h4>Second Order Approximations to the First Derivative</h4> <p>Now let&#39;s look at the following:</p> <p class=math >\[ \delta_{0}u=\frac{u(x+\Delta x)-u(x-\Delta x)}{2\Delta x}. \]</p> <p>The claim is this differencing scheme is second order. To show this, we once again turn to Taylor Series. Let&#39;s do this for both terms:</p> <p class=math >\[ u(x+\Delta x)	=u(x)+\Delta xu^{\prime}(x)+\frac{\Delta x^{2}}{2}u^{\prime\prime}(x)+\mathcal{O}(\Delta x^{3}) \]</p> <p class=math >\[ u(x-\Delta x)	=u(x)-\Delta xu^{\prime}(x)+\frac{\Delta x^{2}}{2}u^{\prime\prime}(x)+\mathcal{O}(\Delta x^{3}) \]</p> <p>Now we subtract the two:</p> <p class=math >\[ u(x+\Delta x)-u(x-\Delta x)=2\Delta xu^{\prime}(x)+\mathcal{O}(\Delta x^{3}) \]</p> <p>and thus we move tems around to get</p> <p class=math >\[ \delta_{0}u=\frac{u(x+\Delta x)-u(x-\Delta x)}{2\Delta x}=u^{\prime}(x)+\mathcal{O}\left(\Delta x^{2}\right) \]</p> <p>What does this improvement mean? Let&#39;s say we go from <span class=math >$\Delta x$</span> to <span class=math >$\frac{\Delta x}{2}$</span>. Then while the error from the first order method is around <span class=math >$\frac{1}{2}$</span> the original error, the error from the central differencing method is <span class=math >$\frac{1}{4}$</span> the original error&#33; When trying to get an accurate solution, this quadratic reduction can make quite a difference in the number of required points.</p> <h4>Second Derivative Central Difference</h4> <p>Now we want a second derivative approximation. Let&#39;s show the classic central difference formula for the second derivative:</p> <p class=math >\[ \delta_{0}^{2}u=\frac{u(x+\Delta x)-2u(x)+u(x-\Delta x)}{\Delta x^{2}} \]</p> <p>is second order. To do so, we expand out the two terms:</p> <p class=math >\[ u(x+\Delta x)	=u(x)+\Delta xu^{\prime}(x)+\frac{\Delta x^{2}}{2}u^{\prime\prime}(x)+\frac{\Delta x^{3}}{6}u^{\prime\prime\prime}(x)+\mathcal{O}\left(\Delta x^{4}\right) \]</p> <p class=math >\[ u(x-\Delta x)	=u(x)-\Delta xu^{\prime}(x)+\frac{\Delta x^{2}}{2}u^{\prime\prime}(x)-\frac{\Delta x^{3}}{6}u^{\prime\prime\prime}(x)+\mathcal{O}\left(\Delta x^{4}\right) \]</p> <p>and now plug it in. It&#39;s clear the <span class=math >$u(x)$</span> cancels out. The opposite signs makes <span class=math >$u^{\prime}(x)$</span> cancel out, and then the same signs and cancellation makes the <span class=math >$u^{\prime\prime}$</span> term have a coefficient of 1. But, the opposite signs makes the <span class=math >$u^{\prime\prime\prime}$</span> term cancel out. Thus when we simplify and divide by <span class=math >$\Delta x^{2}$</span> we get</p> <p class=math >\[ \frac{u(x+\Delta x)-2u(x)+u(x-\Delta x)}{\Delta x^{2}}=u^{\prime\prime}(x)+\mathcal{O}\left(\Delta x^{2}\right). \]</p> <h4>Finite Differencing from Polynomial Interpolation</h4> <p>Finite differencing can also be derived from polynomial interpolation. Draw a line between two points. What is the approximation for the first derivative?</p> <p class=math >\[ \delta_{+}u=\frac{u(x+\Delta x)-u(x)}{\Delta x} \]</p> <p>Now draw a quadratic through three points. i.e., given <span class=math >$u_{1}$</span>, <span class=math >$u_{2}$</span>, and <span class=math >$u_{3}$</span> at <span class=math >$x=0$</span>, <span class=math >$\Delta x$</span>, <span class=math >$2\Delta x$</span>, we want to find the interpolating polynomial</p> <p class=math >\[ g(x)=a_{1}x^{2}+a_{2}x+a_{3} \]</p> <p>.</p> <p>Setting <span class=math >$g(0)=u_{1}$</span>, <span class=math >$g(\Delta x)=u_{2}$</span>, and <span class=math >$g(2\Delta x)=u_{3}$</span>, we get the following relations:</p> <p class=math >\[ u_{1}	=g(0)=a_{3} \]</p> <p class=math >\[ u_{2}	=g(\Delta x)=a_{1}\Delta x^{2}+a_{2}\Delta x+a_{3} \]</p> <p class=math >\[ u_{3}	=g(2\Delta x)=4a_{1}\Delta x^{2}+2a_{2}\Delta x+a_{3} \]</p> <p>which when we write in matrix form is:</p> <p class=math >\[ \left(\begin{array}{ccc} 0 & 0 & 1\\ \Delta x^{2} & \Delta x & 1\\ 4\Delta x^{2} & 2\Delta x & 1 \end{array}\right)\left(\begin{array}{c} a_{1}\\ a_{2}\\ a_{3} \end{array}\right)=\left(\begin{array}{c} u_{1}\\ u_{2}\\ u_{3} \end{array}\right) \]</p> <p>and thus we can invert the matrix to get the a&#39;s:</p> <p class=math >\[ a_{1}	=\frac{u_{3}-2u_{2}+u_{1}}{2\Delta x^{2}} \]</p> <p class=math >\[ a_{2}	=\frac{-u_{3}+4u_{2}-3u_{1}}{2\Delta x} \]</p> <p class=math >\[ a_{3}	=u_{1}\text{ or }g(x)=\frac{u_{3}-2u_{2}-u_{1}}{2\Delta x^{2}}x^{2}+\frac{-u_{3}+4u_{2}-3u_{1}}{2\Delta x}x+u_{1} \]</p> <p>Now we can get derivative approximations from this. Notice for example that</p> <p class=math >\[ g^{\prime}(x)=\frac{u_{3}-2u_{2}+u_{1}}{\Delta x^{2}}x+\frac{-u_{3}+4u_{2}-3u_{1}}{2\Delta x} \]</p> <p>Now what&#39;s the derivative at the middle point?</p> <p class=math >\[ g^{\prime}\left(\Delta x\right)=\frac{u_{3}-2u_{2}+u_{1}}{\Delta x}+\frac{-u_{3}+4u_{2}-3u_{1}}{2\Delta x}=\frac{u_{3}-u_{1}}{2\Delta x}. \]</p> <p>And now check</p> <p class=math >\[ g^{\prime\prime}(\Delta x)=\frac{u_{3}-2u_{2}+u_{1}}{\Delta x^{2}} \]</p> <p>which is the central derivative formula. This gives a systematic way of deriving higher order finite differencing formulas. In fact, this formulation allows one to derive finite difference formulae for non-evenly spaced grids as well&#33; The algorithm which automatically generates stencils from the interpolating polynomial forms is the Fornberg algorithm.</p> <h4>Multidimensional Finite Difference Operations</h4> <p>Now let&#39;s look at the multidimensional Poisson equation, commonly written as:</p> <p class=math >\[ \Delta u = f(x,y) \]</p> <p>where <span class=math >$\Delta u = u_{xx} + u_{yy}$</span>. Using the logic of the previous sections, we can approximate the two derivatives to have:</p> <p class=math >\[ \frac{u(x+\Delta x,y)-2u(x,y)+u(x-\Delta x,y)}{\Delta x^{2}} + \frac{u(x,y+\Delta y)-2u(x,y)+u(x-x,y-\Delta y)}{\Delta y^{2}}=u^{\prime\prime}(x)+\mathcal{O}\left(\Delta x^{2}\right) + \mathcal{O}\left(\Delta y^{2}\right). \]</p> <p>Notice that this is the stencil operation:</p> <pre><code>0  1 0
1 -4 1
0  1 0</code></pre> <p>This means that <strong>derivative discretizations are stencil or convolutional operations</strong>.</p> <h2>Representation and Implementation of Stencil Operations</h2> <h3>Stencil Operations as Sparse Matrices</h3> <p>Stencil operations are linear operators, i.e. <span class=math >$S[x+\alpha y] = S[x] + \alpha S[y]$</span> for any sufficiently nice stencil operation <span class=math >$S$</span> &#40;note &quot;sufficiently nice&quot;: there is a &quot;stencil&quot; operation mentioned in the convolutional neural networks section which was not linear: which operation was it?&#41;. Now we write these operators as matrices. Notice that for the vector:</p> <p class=math >\[ U=\left(\begin{array}{c} u_{1}\\ \vdots\\ u_{n} \end{array}\right), \]</p> <p>we have that</p> <p class=math >\[ \delta_{+}U=\left(\begin{array}{c} u_{2}-u_{1}\\ \vdots\\ u_{n}-u_{n-1} \end{array}\right) \]</p> <p>and so</p> <p class=math >\[ \delta_{+}=\left(\begin{array}{ccccc} -1 & 1\\ & -1 & 1\\ & & \ddots & \ddots\\ & & & -1 & 1 \end{array}\right) \]</p> <p>We can do the same to understand the other operators. But notice something: this operator isn&#39;t square&#33; In order for this to be square, in order to know what happens at the endpoint, we need to know the boundary conditions. I.e., an assumption on the value or derivative at <span class=math >$u(0)$</span> or <span class=math >$u(1)$</span> is required in order to get the first/last rows of the matrix&#33;</p> <p>Similarly, <span class=math >$\delta_{0}^{2}$</span> can be represented as the tridiagonal matrix of <code>&#91;1 -2 1&#93;</code>, also known as the Strang matrix.</p> <p>Now let&#39;s think about the higher dimensional forms as a vector, i.e. <code>vec&#40;u&#41;</code>. In this case, what is the matrix <code>A</code> for which <code>reshape&#40;A*vec&#40;u&#41;,size&#40;u&#41;...&#41;</code> performs the higher dimensional Laplacian, i.e. <span class=math >$u_{xx} + u_{yy}$</span>? The answer is that it discretizes via Kronecker products to:</p> <p class=math >\[ A=I_{y}\otimes A_{x}+A_{y}\otimes I_{x} \]</p> <p>or:</p> <p class=math >\[ \frac{\partial^{2}}{\partial x^{2}}=\left(\begin{array}{cccc} A_{x}\\ & A_{x}\\ & & \ddots\\ & & & A_{x} \end{array}\right) \]</p> <p>and</p> <p class=math >\[ \frac{\partial^{2}}{\partial y^{2}}=\left(\begin{array}{cccc} -2I_{x} & I_{x}\\ I_{x} & -2I_{x} & I_{x}\\ & & \ddots\\ \\ \end{array}\right) \]</p> <p>To see why this is the case, understand it again as the stencil operation</p> <pre><code>0  1 0
1 -4 1
0  1 0</code></pre> <p>In this operation, at a point you still use the up and down neighbors, and thus this has a tridiagonal form since those are the immediate neighbors, but the next <span class=math >$y$</span> value is <span class=math >$N$</span> over, so this is where the block tridiagonal form comes for the stencil in the <span class=math >$y$</span> terms. When these are added together one receives the appropriate matrix. The Kronecker product effectively encodes this &quot;N over&quot; behavior. It also readily generalizes to <span class=math >$N$</span> dimensions. To see this for 3-dimensional Laplacians, <span class=math >$u_{xx} + u_{yy} + u_{zz}$</span>, notice that</p> <p class=math >\[ A=I_z \otimes I_{y}\otimes A_{x} + I_z \otimes A_{y}\otimes I_{x} + A_z \otimes I_y \otimes I_x \]</p> <p>using the same reasoning about &quot;N&quot; over and &quot;N^2 over&quot;, and from this formulation it&#39;s clear how to generalize to arbitrary dimensions.</p> <p>We note that there is an alternative representation as well for 2D forms. We can represent them with left and right matrix operations. When <span class=math >$u$</span> is represented as a matrix, notice that</p> <p class=math >\[ A(u) = A_y u + u A_x \]</p> <p>where <span class=math >$A_y$</span> and <span class=math >$A_x$</span> are both the <code>&#91;1 -2 1&#93;</code> 1D tridiagonal stencil matrix, but by right multiplying it&#39;s occurring along the columns and left multiplying occurs along the rows. This then gives a semi-dense formulation of the stencil operation.</p> <h3>Implementation via Stencil Compilers</h3> <p>Sparse matrix implementations of stencils are fairly inefficient given the way that sparse matrices are represented &#40;lists of &#40;i,j,v&#41; pairs, which are then compressed into CSR or CSC formats&#41;. However, it moves in the right direction by noticing that the operation</p> <pre><code>u&#91;i&#43;1,j&#93; &#43; u&#91;i,j&#43;1&#93; &#43; u&#91;i-1,j&#93; &#43; u&#91;i,j-1&#93; - 4u&#91;i,j&#93;</code></pre>
<p>is an inefficient way to walk through the data. The reason is because <code>u&#91;i,j&#43;1&#93;</code> is using values that are far away from <code>u&#91;i,j&#93;</code>, and thus they may not necessarily be in the cache.</p>
<p>Thus what is generally used is a <em>stencil compiler</em> which generates functions for stencil operations. These work by dividing the tensor into blocks on which the stencil is applied, where the blocks are small enough to allow the cache lines to fit the future points. This is a very deep computational topic that is beyond the scope of this course. Note one of the main reasons why NVIDA&#39;s CUDA dominates machine learning is because of its <code>cudnn</code> library, which is a very efficient GPU stencil computation library that is specifically tuned to NVIDIA&#39;s GPUs.</p>
<h2>Cross-Discipline Learning</h2>
<p>Given these relations, there is a lot each of the disciplines can learn from one another about stencil computations.</p>
<h3>What ML can learning from SciComp: Stability of Convolutional RNNs</h3>
<p>Stability of time-dependent partial differential equations is a long-known problem. Stability of an RNN defined by stencil computations is then stability of Euler discretizations of PDEs. Let&#39;s take a look at Von Neumann analysis of PDE stability.</p>
<p>Let&#39;s look at the error update equation. Write</p>
<p class=math >\[
e_{i}^{n}=u(x_{j},t_{n})-u_{j}^{n}
\]</p>
<p>For <span class=math >$e_{i}^{n}$</span>, as before, plug it in, add and subtract <span class=math >$u(x_{j},t^{n})=u_{j}^{n}$</span>, and then we get</p>
<p class=math >\[
e_{i}^{n+1}=e_{i}^{n}+\mu\left(e_{i+1}^{n}-2e_{i}^{n}+e_{i-1}^{n}\right)+\Delta t\tau_{i}^{n}
\]</p>
<p>where</p>
<p class=math >\[
\tau_{i}^{n}\sim\mathcal{O}(\Delta t)+\mathcal{O}(\Delta x^{2}).
\]</p>
<p>Stability requires that the homogenous equation goes to zero. Another way of saying that is that the propagation of errors has errors decrease their influence over time. Thus we look at:</p>
<p class=math >\[
e_{i}^{n+1}	=e_{i}^{n}+\mu\left(e_{i+1}^{n}-2e_{i}^{n}+e_{i-1}^{n}\right) =\left(1-2\mu\right)e_{i}^{n}+\mu e_{i+1}^{n}+\mu e_{i-1}^{n}
\]</p>
<p>A necessary condition for decreasing is then for all coefficients to be positive</p>
<p class=math >\[
1-2\mu\geq0
\]</p>
<p>or</p>
<p class=math >\[
\mu\leq\frac{1}{2}
\]</p>
<p>A more satisfying way may be to look at the generated ODE</p>
<p class=math >\[
u^{\prime}=Au
\]</p>
<p>where A is the matrix <span class=math >$\left[\mu,1-2\mu,\mu\right].$</span></p>
<p>But  finding the maximum eigenvalue is non-trivial. But for linear PDEs, one nice way to analyze the stability directly is to use the Fourier mode decomposition. This is known as Van Neumann stability analysis. To do this, decompose <span class=math >$U$</span> into the Fourier modes:</p>
<p class=math >\[
U(x,t)=\sum_{k}\hat{U}(t)e^{ikx}
\]</p>
<p>Since</p>
<p class=math >\[
x_{j}=j\Delta x,
\]</p>
<p>we can write this out as</p>
<p class=math >\[
U_{j}^{n}=\hat{U}^{n}e^{ikj\Delta x}
\]</p>
<p>and then plugging this into the FTCS scheme we get</p>
<p class=math >\[
\frac{\hat{U}^{n+1}e^{ikj\Delta x}-\hat{U}^{n}e^{ikj\Delta x}}{\Delta t}=\frac{\hat{U}^{n}e^{ik(j+1)\Delta x}-2\hat{U}^{n}e^{ikj\Delta x}+\hat{U}^{n}e^{ik(j-1)\Delta x}}{\Delta x^{2}}
\]</p>
<p>Let G be the growth factor, defined as</p>
<p class=math >\[
G=\frac{\hat{U}^{n+1}}{\hat{U}^{n}}
\]</p>
<p>and thus after cancelling we get</p>
<p class=math >\[
\frac{G-1}{\Delta t}=\frac{e^{ik\Delta x}-2+e^{-ik\Delta x}}{\Delta x^{2}}
\]</p>
<p>Since</p>
<p class=math >\[
e^{ik\Delta x}+e^{-ik\Delta x}=2\cos\left(k\Delta x\right),
\]</p>
<p>then we get</p>
<p class=math >\[
G=1-\mu\left(2\cos\left(k\Delta x\right)-2\right)
\]</p>
<p>and using the half angle formula</p>
<p class=math >\[
G=1-4\mu\sin^{2}\left(\frac{k\Delta x}{2}\right)
\]</p>
<p>In order to be stable, we require</p>
<p class=math >\[
\left|G\right|\leq1,
\]</p>
<p>which means</p>
<p class=math >\[
-1\leq1-4\mu\sin^{2}\left(\frac{k\Delta x}{2}\right)\leq1 \mu>0
\]</p>
<p>and so <span class=math >$\leq1$</span> is simple. Since <span class=math >$\sin^{2}(x)\leq1$</span>, then we can simplify this to</p>
<p class=math >\[
-1\leq1-4\mu
\]</p>
<p>and thus <span class=math >$\mu\leq\frac{1}{2}$</span>. With backwards Euler we get</p>
<p class=math >\[
\frac{G-1}{\Delta t}=\frac{G}{\Delta x^{2}}\left(e^{ik\Delta x}-2+e^{-ik\Delta x}\right)
\]</p>
<p>and thus get</p>
<p class=math >\[
G+4G\mu\sin^{2}\left(\frac{k\Delta x}{2}\right)=1
\]</p>
<p>and thus</p>
<p class=math >\[
G=\frac{1}{1+4\mu\sin^{2}\left(\frac{k\Delta x}{2}\right)}\leq1.
\]</p>
<h3>What SciComp can learn from ML: Moderate Generalizations to Partial Differential Equation Models</h3>
<p>Instead of using</p>
<p class=math >\[
\Delta u = f
\]</p>
<p>we can start with</p>
<p class=math >\[
S[u] = f
\]</p>
<p>a stencil computation, predefined to match a known partial differential equation operator, and then <em>transfer learn</em> the stencil to better match data. This is an approach which is starting to move down the lines of <em>physics-informed machine learning</em> that will be further explored in future lectures.</p>


<div class=footer >
  <p>
    Published from <a href=pdes_and_convolutions.jmd >pdes_and_convolutions.jmd</a>
    using <a href="http://github.com/JunoLab/Weave.jl">Weave.jl</a> v0.10.12 on 2025-02-03.
  </p>
</div>

<div class=back-to-top >
  <span><a href="#" title="Back to Top"><i class="fa fa-chevron-circle-up"></i></a></span>
</div>


</div>
        </div> 
    </div>