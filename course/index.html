<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/tufte.css"> <link rel=stylesheet  href="/css/latex.css"> <link rel=stylesheet  href="/css/adjust.css"> <link rel=icon  href="/assets/favicon.png"> <link rel=stylesheet  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <title>Course Overview - MIT Parallel Computing and Scientific Machine Learning (SciML)</title> <div id=layout > <div id=menu > <ul> <li><a style="font-size:larger;" href=https://github.com/SciML/SciMLBook><i class="fa fa-github"></i></a> <li><a style="font-size:larger;" href="/">Home</a> <li><a style="font-size:larger;" href="/course/">Course</a> <li><a style="font-size:larger;" href="/homework/">Homework</a> <li><a style="font-size:larger;" href="/lectures/">Lectures</a> <li><a style="font-size:larger;" href="/notes/">Notes</a> <ul style="font-size:smaller"> <li><a href="/notes/02-Optimizing_Serial_Code/">02: Serial Code</a> <li><a href="/notes/03-Introduction_to_Scientific_Machine_Learning_through_Physics-Informed_Neural_Networks/">03: SciML Intro</a> <li><a href="/notes/04-How_Loops_Work-An_Introduction_to_Discrete_Dynamics/">04: How Loops Work</a> <li><a href="/notes/05-The_Basics_of_Single_Node_Parallel_Computing/">05: Basics of Parallelism</a> <li><a href="/notes/06-The_Different_Flavors_of_Parallelism/">06: Flavors of Parallelism</a> <li><a href="/notes/07-Ordinary_Differential_Equations-Applications_and_Discretizations/">07: ODEs</a> <li><a href="/notes/08-Forward-Mode_Automatic_Differentiation_(AD)_via_High_Dimensional_Algebras/">08: Forward AD</a> <li><a href="/notes/09-Solving_Stiff_Ordinary_Differential_Equations/">09: Stiff ODEs</a> <li><a href="/notes/10-Basic_Parameter_Estimation-Reverse-Mode_AD-and_Inverse_Problems/">10: Reverse AD</a> <li><a href="/notes/11-Differentiable_Programming_and_Neural_Differential_Equations/">11: Î´P</a> <li><a href="/notes/12-Description_of_MPI_and_MPI/">12: MPI</a> <li><a href="/notes/13-GPU_programming/">13: GPUs</a> <li><a href="/notes/14-PDEs_Convolutions_and_the_Mathematics_of_Locality/">14: PDEs</a> <li><a href="/notes/15-Mixing_Differential_Equations_and_Neural_Networks_for_Physics-Informed_Learning/">15: Physics Informed Learning</a> <li><a href="/notes/16-From_Optimization_to_Probabilistic_Programming/">16: Probabilistic Programming</a> <li><a href="/notes/17-Global_Sensitivity_Analysis/">17: Global Sensitivity Analysis</a> <li><a href="/notes/18-Code_Profiling_and_Optimization/">18: Profiling & Optimization</a> <li><a href="/notes/19-Uncertainty_Programming-Generalized_Uncertainty_Quantification/">19: Uncertainty Programming</a> </ul> </ul> </div> <div id=main > <div class=franklin-content > <h1 id=course_overview ><a href="#course_overview" class=header-anchor >Course Overview</a></h1> <h2 id=syllabus ><a href="#syllabus" class=header-anchor >Syllabus</a></h2> <p><strong>Pre-recorded online lectures are available to complement the lecture notes</strong></p> <p><strong>Prerequisites</strong>: While this course will be mixing ideas from high performance computing, numerical analysis, and machine learning, no one in the course is expected to have covered all of these topics before. Understanding of calculus, linear algebra, and programming is essential. 18.337 is a graduate-level subject so mathematical maturity and the ability to learn from primary literature is necessary. Problem sets will involve use of <a href="http://julialang.org/">Julia</a>, a Matlab-like environment &#40;little or no prior experience required; you will learn as you go&#41;.</p> <p><strong>Textbook &amp; Other Reading</strong>: There is no textbook for this course or the field of scientific machine learning. Some helpful resources are <a href="https://www.springer.com/gp/book/9783540566700">Hairer and Wanner&#39;s Solving Ordinary Differential Equations I &amp; II</a> and <a href="https://www.amazon.com/Computational-Science-Engineering-Gilbert-Strang/dp/0961408812">Gilbert Strang&#39;s Computational Science and Engineering</a>. Much of the reading will come in the form of primary literature from journal articles posted here.</p> <h3 id=schedule_of_topics ><a href="#schedule_of_topics" class=header-anchor >Schedule of Topics</a></h3> <p>Each topic is a group of three pieces: a numerical method, a performance-engineering technique, and a scientific application. These three together form a complete usable program that is demonstrated.</p> <ul> <li><p>The basics of scientific simulators &#40;Week 1-2&#41;</p> <ul> <li><p>What is Scientific Machine Learning?</p> <li><p>Optimization of serial code.</p> <li><p>Introduction to discrete and continuous dynamical systems.</p> </ul> <li><p>Introduction to Parallel Computing &#40;Week 2-3&#41;</p> <ul> <li><p>Forms of parallelism and applications</p> <li><p>Parallelizing differential equation solvers</p> <li><p>Optimal local parallelism via multithreading</p> <li><p>Linear Algebra libraries you should know</p> </ul> </ul> <p>Homework 1: Parallelized dynamical system simulations and ODE integrators</p> <ul> <li><p>Continuous Dynamics &#40;Week 4&#41;</p> <ul> <li><p>Ordinary differential equations as the language for ecology, Newtonian mechanics, and beyond.</p> <li><p>Numerical methods for non-stiff ordinary differential equations</p> <li><p>Definition of stiffness</p> <li><p>Efficiently solving stiff ordinary differential equations</p> <li><p>Stiff differential equations arising from biochemical interactions in developmental biology and ecology</p> <li><p>Utilizing type systems and generic algorithms as a mathematical tool</p> <li><p>Forward-mode automatic differentiation for solving f&#40;x&#41;&#61;0</p> <li><p>Matrix coloring and sparse differentiation</p> </ul> </ul> <p>Homework 2: Parameter estimation in dynamical systems and overhead of parallelism</p> <ul> <li><p>Inverse problems and Differentiable Programming &#40;Week 6&#41;</p> <ul> <li><p>Definition of inverse problems with applications to clinical pharmacology and smartgrid optimization</p> <li><p>Adjoint methods for fast gradients</p> <li><p>Automated adjoints through reverse-mode automatic differentiation &#40;backpropagation&#41;</p> <li><p>Adjoints of differential equations</p> <li><p>Using neural ordinary differential equations as a memory-efficient RNN for deep learning</p> </ul> <li><p>Neural networks, and array-based parallelism &#40;Week 8&#41;</p> <ul> <li><p>Cache optimization in numerical linear algebra</p> <li><p>Parallelism through array operations</p> <li><p>How to optimize algorithms for GPUs</p> </ul> <li><p>Distributed parallel computing &#40;Jeremy Kepner: Weeks 7-8&#41;</p> <ul> <li><p>Forms of parallelism</p> <li><p>Using distributed computing vs multithreading</p> <li><p>Message passing and deadlock</p> <li><p>Map-Reduce as a framework for distributed parallelism</p> <li><p>Implementing distributed parallel algorithms with MPI</p> </ul> </ul> <p>Homework 3: Training neural ordinary differential equations &#40;with GPUs&#41;</p> <ul> <li><p>Physics-Informed Neural Networks and Neural Differential Equations &#40;Week 9-10&#41;</p> <ul> <li><p>Automatic discovery of differential equations</p> <li><p>Solving differential equations with neural networks</p> <li><p>Discretizations of PDEs</p> <li><p>Basics of neural networks and definitions</p> <li><p>The relationship between convolutional neural networks and PDEs</p> </ul> <li><p>Probabilistic Programming, AKA Bayesian Estimation on Programs &#40;Week 10-11&#41;</p> <ul> <li><p>The connection between optimization and Bayesian methods: Bayesian posteriors vs MAP optimization</p> <li><p>Introduction to Markov-Chain Monte Carlo methods</p> <li><p>Hamiltonian Monte Carlo is just a symplectic ODE solver</p> <li><p>Uncertainty quantification of parameter estimates through posteriors</p> </ul> <li><p>Globalizing the understanding of models &#40;Week 11-12&#41;</p> <ul> <li><p>Global sensitivity analysis</p> <li><p>Global optimization</p> <li><p>Surrogate Modeling</p> <li><p>Uncertainty Quantification</p> </ul> </ul> <h3 id=homeworks ><a href="#homeworks" class=header-anchor >Homeworks</a></h3> <ul> <li><p><a href="/homework/01/">Homework 1: Parallelized Dynamics. Due October 1st</a></p> <li><p><a href="/homework/02/">Homework 2: Parameter Estimation in Dynamical Systems and Bandwidth Maximization. Due November 5th</a></p> <li><p><a href="/homework/03/">Homework 3: Neural Ordinary Differential Equation Adjoints. Due December 9th</a></p> </ul> <h3 id=lecture_summaries_and_handouts ><a href="#lecture_summaries_and_handouts" class=header-anchor >Lecture Summaries and Handouts</a></h3> <p>Note that lectures are broken down by topic, not by day. Some lectures are more than 1 class day, others are less.</p> <div class=back-to-top > <span><a href="#" title="Back to Top"><i class="fa fa-chevron-circle-up"></i></a></span> </div> <div class=page-foot > <div class=copyright > <a href=https://github.com/SciML/SciMLBook>SciML Book source code</a> <br> &copy; Chris Rackauckas. Last modified: July 09, 2023. <br> Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> </div> </div>