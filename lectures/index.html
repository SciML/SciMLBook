<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/tufte.css"> <link rel=stylesheet  href="/css/latex.css"> <link rel=stylesheet  href="/css/adjust.css"> <link rel=icon  href="/assets/favicon.png"> <link rel=stylesheet  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <title>Lecture Overview</title> <div id=layout > <div id=menu > <ul> <li><a style="font-size:larger;" href=https://github.com/SciML/SciMLBook><i class="fa fa-github"></i></a> <li><a style="font-size:larger;" href="/">Home</a> <li><a style="font-size:larger;" href="/course/">Course</a> <li><a style="font-size:larger;" href="/homework/">Homework</a> <li><a style="font-size:larger;" href="/lectures/">Lectures</a> <li><a style="font-size:larger;" href="/notes/">Notes</a> <ul style="font-size:smaller"> <li><a href="/notes/02/">02</a> <li><a href="/notes/03/">03</a> <li><a href="/notes/04/">04</a> <li><a href="/notes/05/">05</a> <li><a href="/notes/06/">06</a> <li><a href="/notes/07/">07</a> <li><a href="/notes/08/">08</a> <li><a href="/notes/09/">09</a> <li><a href="/notes/10/">10</a> <li><a href="/notes/11/">11</a> <li><a href="/notes/13/">13</a> <li><a href="/notes/14/">14</a> <li><a href="/notes/15/">15</a> <li><a href="/notes/16/">16</a> <li><a href="/notes/17/">17</a> <li><a href="/notes/18/">18</a> <li><a href="/notes/19/">19</a> </ul> </ul> </div> <div id=main > <div class=franklin-content > <h1 id=lecture_overview ><a href="#lecture_overview" class=header-anchor >Lecture Overview</a></h1> <div class=franklin-toc ><ol><li><a href="#lecture_1_introduction_and_syllabus">Lecture 1: Introduction and Syllabus</a><ol><li><a href="#lecture_10_introduction">Lecture 1.0: Introduction</a><ol><li><a href="#lecture_and_notes">Lecture and Notes</a></ol><li><a href="#lecture_11_getting_started_with_julia">Lecture 1.1: Getting Started with Julia</a><ol><li><a href="#lecture_and_notes__2">Lecture and Notes</a><li><a href="#optional_extra_resources">Optional Extra Resources</a></ol></ol><li><a href="#lecture_2_optimizing_serial_code">Lecture 2: Optimizing Serial Code</a><ol><li><a href="#lecture_and_notes__3">Lecture and Notes</a><li><a href="#optional_extra_resources__2">Optional Extra Resources</a></ol><li><a href="#lecture_3_introduction_to_scientific_machine_learning_through_physics-informed_neural_networks">Lecture 3: Introduction to Scientific Machine Learning Through Physics-Informed Neural Networks</a><ol><li><a href="#optional_extra_resources__3">Optional Extra Resources</a></ol><li><a href="#lecture_4_introduction_to_discrete_dynamical_systems">Lecture 4: Introduction to Discrete Dynamical Systems</a><ol><li><a href="#optional_extra_resources__4">Optional Extra Resources</a></ol><li><a href="#lecture_5_array-based_parallelism_embarrassingly_parallel_problems_and_data-parallelism_the_basics_of_single_node_parallel_computing">Lecture 5: Array-Based Parallelism, Embarrassingly Parallel Problems, and Data-Parallelism: The Basics of Single Node Parallel Computing</a><ol><li><a href="#optional_extra_resources__5">Optional Extra Resources</a></ol><li><a href="#lecture_6_styles_of_parallelism">Lecture 6: Styles of Parallelism</a><li><a href="#lecture_7_ordinary_differential_equations_applications_and_discretizations">Lecture 7: Ordinary Differential Equations: Applications and Discretizations</a><li><a href="#lecture_8_forward-mode_automatic_differentiation">Lecture 8: Forward-Mode Automatic Differentiation</a><li><a href="#lecture_9_solving_stiff_ordinary_differential_equations">Lecture 9: Solving Stiff Ordinary Differential Equations</a><ol><li><a href="#lecture_notes">Lecture Notes</a><li><a href="#additional_readings_on_convergence_of_newtons_method">Additional Readings on Convergence of Newton&#39;s Method</a></ol><li><a href="#lecture_10_basic_parameter_estimation_reverse-mode_ad_and_inverse_problems">Lecture 10: Basic Parameter Estimation, Reverse-Mode AD, and Inverse Problems</a><li><a href="#lecture_11_differentiable_programming_and_neural_differential_equations">Lecture 11: Differentiable Programming and Neural Differential Equations</a><ol><li><a href="#additional_readings_on_ad_implementations">Additional Readings on AD Implementations</a></ol><li><a href="#lecture_12_sciml_in_practice">Lecture 12: SciML in Practice</a><ol><li><a href="#lecture_121_mpi_for_distributed_computing">Lecture 12.1: MPI for Distributed Computing</a><li><a href="#lecture_122_mathematics_of_machine_learning_and_high_performance_computing">Lecture 12.2: Mathematics of Machine Learning and High Performance Computing</a></ol><li><a href="#lecture_13_gpu_computing">Lecture 13: GPU Computing</a><li><a href="#lecture_14_partial_differential_equations_and_convolutional_neural_networks">Lecture 14: Partial Differential Equations and Convolutional Neural Networks</a><ol><li><a href="#additional_readings">Additional Readings</a></ol><li><a href="#lecture_15_more_algorithms_which_connect_differential_equations_and_machine_learning">Lecture 15: More Algorithms which Connect Differential Equations and Machine Learning</a><li><a href="#lecture_16_probabilistic_programming">Lecture 16: Probabilistic Programming</a><li><a href="#lecture_17_global_sensitivity_analysis">Lecture 17: Global Sensitivity Analysis</a><li><a href="#lecture_18_code_profiling_and_optimization">Lecture 18: Code Profiling and Optimization</a><li><a href="#lecture_19_uncertainty_programming_and_generalized_uncertainty_quantification">Lecture 19: Uncertainty Programming and Generalized Uncertainty Quantification</a><li><a href="#final_project">Final Project</a></ol></div> <h2 id=lecture_1_introduction_and_syllabus ><a href="#lecture_1_introduction_and_syllabus" class=header-anchor >Lecture 1: Introduction and Syllabus</a></h2> <h3 id=lecture_10_introduction ><a href="#lecture_10_introduction" class=header-anchor >Lecture 1.0: Introduction</a></h3> <h4 id=lecture_and_notes ><a href="#lecture_and_notes" class=header-anchor >Lecture and Notes</a></h4> <ul> <li><p><a href="https://youtu.be/3IoqyXmAAkU">Introduction and Syllabus &#40;Lecture&#41;</a></p> </ul> <p>This is to make sure we&#39;re all on the same page. It goes over the syllabus and what will be expected of you throughout the course. If you have not joined the Slack, please use the link from the introduction email &#40;or email me if you need the link&#33;&#41;.</p> <h3 id=lecture_11_getting_started_with_julia ><a href="#lecture_11_getting_started_with_julia" class=header-anchor >Lecture 1.1: Getting Started with Julia</a></h3> <h4 id=lecture_and_notes__2 ><a href="#lecture_and_notes__2" class=header-anchor >Lecture and Notes</a></h4> <ul> <li><p><a href="https://youtu.be/-lJK92bEKow">Getting Started with Julia for Experienced Programmers &#40;Lecture&#41;</a></p> <li><p><a href="https://github.com/mitmath/julia-mit">Julia for Numerical Computation in MIT Courses</a></p> <li><p><a href="https://mit.zoom.us/rec/play/E4zN_2MXQmCjX12admWsmsbG6hIlWJztnMmFjlfDEBnlAj8V2qisRn-CLI_WVnUGJFZ4bV6JGM-41m-u.LeAWxiLriV5HwqBK?startTime&#61;1599594382000">Steven Johnson: MIT Julia Tutorial</a></p> </ul> <h4 id=optional_extra_resources ><a href="#optional_extra_resources" class=header-anchor >Optional Extra Resources</a></h4> <p>If you are not comfortable with Julia yet, here&#39;s a few resources as sort of a &quot;crash course&quot; to get you up an running:</p> <ul> <li><p><a href="https://docs.julialang.org/en/v1/">The Julia Manual</a></p> <li><p><a href="https://youtu.be/QVmU29rCjaA">Developing Julia Packages</a></p> <li><p><a href="https://www.youtube.com/watch?v&#61;8h8rQyEpiZA">Julia Tutorial &#40;Youtube Video by Jane Herriman&#41;</a></p> </ul> <p>Some deeper materials:</p> <ul> <li><p><a href="https://benlauwens.github.io/ThinkJulia.jl/latest/book.html">ThinkJulia</a></p> <li><p><a href="https://en.wikibooks.org/wiki/Introducing_Julia">Julia Wikibook</a></p> <li><p><a href="http://ucidatascienceinitiative.github.io/IntroToJulia/">Intro To Julia for Data Science and Scientific Computing &#40;With Problems and Answers&#41;</a></p> <li><p><a href="https://cheatsheets.quantecon.org/">QuantEcon Cheatsheet</a></p> <li><p><a href="https://docs.julialang.org/en/v1/manual/noteworthy-differences/">Julia Noteworthy Differences from Other Languages</a></p> </ul> <p>Steven Johnson will be running a Julia workshop on 9/8/2020 for people who are interested. More details TBA.</p> <h2 id=lecture_2_optimizing_serial_code ><a href="#lecture_2_optimizing_serial_code" class=header-anchor >Lecture 2: Optimizing Serial Code</a></h2> <h3 id=lecture_and_notes__3 ><a href="#lecture_and_notes__3" class=header-anchor >Lecture and Notes</a></h3> <ul> <li><p><a href="https://youtu.be/M2i7sSRcSIw">Optimizing Serial Code in Julia 1: Memory Models, Mutation, and Vectorization &#40;Lecture&#41;</a></p> <li><p><a href="https://youtu.be/10_Ukm9wr9g">Optimizing Serial Code in Julia 2: Type inference, function specialization, and dispatch &#40;Lecture&#41;</a></p> <li><p><a href="/notes/02/">Optimizing Serial Code &#40;Notes&#41;</a></p> </ul> <h3 id=optional_extra_resources__2 ><a href="#optional_extra_resources__2" class=header-anchor >Optional Extra Resources</a></h3> <ul> <li><p><a href="https://tutorials.sciml.ai/html/introduction/03-optimizing_diffeq_code.html">Optimizing Your DiffEq Code</a></p> <li><p><a href="https://www.stochasticlifestyle.com/type-dispatch-design-post-object-oriented-programming-julia/">Type-Dispatch Design: Post Object-Oriented Programming for Julia</a></p> <li><p><a href="https://www.youtube.com/watch?v&#61;r-TLSBdHe1A0">Performance Matters</a></p> <li><p><a href="http://phk.freebsd.dk/B-Heap/queue.html">You&#39;re doing it wrong &#40;B-heaps vs Binary Heaps and Big O&#41;</a></p> <li><p><a href="https://www.youtube.com/watch?v&#61;YQs6IC-vgmo">Bjarne Stroustrup: Why you should avoid Linked Lists</a></p> <li><p><a href="https://biojulia.net/post/hardware/">What scientists must know about hardware to write fast code</a></p> <li><p><a href="https://nullprogram.com/blog/2018/05/27/">When FFI Function Calls Beat Native C &#40;How JIT compilation allows C-calls to be faster than C&#41;</a></p> </ul> <p>Before we start to parallelize code, build huge models, and automatically learn physics, we need to make sure our code is &quot;good&quot;. How do you know you&#39;re writing &quot;good&quot; code? That&#39;s what this lecture seeks to answer. In this lecture we&#39;ll go through the techniques for writing good serial code and checking that your code is efficient.</p> <h2 id=lecture_3_introduction_to_scientific_machine_learning_through_physics-informed_neural_networks ><a href="#lecture_3_introduction_to_scientific_machine_learning_through_physics-informed_neural_networks" class=header-anchor >Lecture 3: Introduction to Scientific Machine Learning Through Physics-Informed Neural Networks</a></h2> <ul> <li><p><a href="https://youtu.be/C3vf9ZFYbjI">Introduction to Scientific Machine Learning 1: Deep Learning as Function Approximation &#40;Lecture&#41;</a></p> <li><p><a href="https://youtu.be/hKHl68Fdpq4">Introduction to Scientific Machine Learning 2: Physics-Informed Neural Networks &#40;Lecture&#41;</a></p> <li><p><a href="/notes/03/">Introduction to Scientific Machine Learning through Physics-Informed Neural Networks &#40;Notes&#41;</a></p> </ul> <h3 id=optional_extra_resources__3 ><a href="#optional_extra_resources__3" class=header-anchor >Optional Extra Resources</a></h3> <ul> <li><p><a href="https://www.youtube.com/watch?v&#61;QwVO0Xh2Hbg">Doing Scientific Machine Learning &#40;4 hour workshop&#41;</a></p> <li><p><a href="https://www.youtube.com/watch?v&#61;5zaB1B4hOnQ">Universal Differential Equations for Scientific Machine Learning</a></p> <li><p><a href="https://www.youtube.com/watch?v&#61;Bk4PJnjuPps">JuliaCon 2020 | Keynote: Scientific Machine Learning | Prof Karen Willcox &#40;High Level&#41;</a></p> <li><p><a href="https://www.osti.gov/servlets/purl/1478744">DOE Workshop Report on Basic Research Needs for Scientific Machine Learning</a></p> </ul> <p>Now let&#39;s take our first stab at the application: scientific machine learning. What is scientific machine learning? We will define the field by looking at a few approaches people are taking and what kinds of problems are being solved using scientific machine learning. The field of scientific machine learning and its span across computational science to applications in climate modeling and aerospace will be introduced. The methodologies that will be studied, in their various names, will be introduced, and the general formula that is arising in the discipline will be laid out: a mixture of scientific simulation tools like differential equations with machine learning primitives like neural networks, tied together through differentiable programming to achieve results that were previously not possible. After doing a survey, we while dive straight into developing a physics-informed neural network solver which solves an ordinary differential equation.</p> <h2 id=lecture_4_introduction_to_discrete_dynamical_systems ><a href="#lecture_4_introduction_to_discrete_dynamical_systems" class=header-anchor >Lecture 4: Introduction to Discrete Dynamical Systems</a></h2> <ul> <li><p><a href="https://www.youtube.com/watch?v&#61;GhBARuHEydM">How Loops Work 1: An Introduction to the Theory of Discrete Dynamical Systems &#40;Lecture&#41;</a></p> <li><p><a href="https://youtu.be/AXHLyHfyEuA">How Loops Work 2: Computationally-Efficient Discrete Dynamics &#40;Lecture&#41;</a></p> <li><p><a href="/notes/04/">How Loops Work, An Introduction to Discrete Dynamics &#40;Notes&#41;</a></p> </ul> <h3 id=optional_extra_resources__4 ><a href="#optional_extra_resources__4" class=header-anchor >Optional Extra Resources</a></h3> <ul> <li><p><a href="https://www.amazon.com/Nonlinear-Dynamics-Chaos-Applications-Nonlinearity/dp/0738204536">Strogatz: Nonlinear Dynamics and Chaos</a></p> <li><p><a href="https://mathinsight.org/equilibria_discrete_dynamical_systems_stability">Stability of discrete dynamics equilibrium</a></p> <li><p><a href="https://chrisrackauckas.com/assets/Papers/ChrisRackauckas-ContinuousDynamics.pdf">Behavior of continuous linear dynamical systems</a></p> <li><p><a href="https://github.com/facebook/folly/blob/master/folly/docs/FBVector.md">Golden Ratio Growth Argument</a></p> <li><p><a href="https://github.com/JuliaLang/julia/pull/16305">Golden Ratio Growth PR and timings</a></p> </ul> <p>Now that the stage is set, we see that to go deeper we will need a good grasp on how both discrete and continuous dynamical systems work. We will start by developing the basics of our scientific simulators: differential and difference equations. A quick overview of geometric results in the study of differential and difference equations will set the stage for understanding nonlinear dynamics, which we will quickly turn to numerical methods to visualize. Even if there is not analytical solution to the dynamical system, overarching behavior such as convergence to zero can be determined through asymptotic means and linearization. We will see later that these same techniques for the basis for the analysis of numerical methods for differential equations, such as the Runge-Kutta and Adams-Bashforth methods.</p> <p>Since the discretization of differential equations is indeed a discrete dynamical system, we will use this as a case study to see how serial scalar-heavy codes should be optimized. SIMD, in-place operations, broadcasting, heap allocations, and static arrays will be used to get fast codes for dynamical system simulation. These simulations will then be used to reveal some intriguing properties of dynamical systems which will be further explored through the rest of the course.</p> <h2 id=lecture_5_array-based_parallelism_embarrassingly_parallel_problems_and_data-parallelism_the_basics_of_single_node_parallel_computing ><a href="#lecture_5_array-based_parallelism_embarrassingly_parallel_problems_and_data-parallelism_the_basics_of_single_node_parallel_computing" class=header-anchor >Lecture 5: Array-Based Parallelism, Embarrassingly Parallel Problems, and Data-Parallelism: The Basics of Single Node Parallel Computing</a></h2> <ul> <li><p><a href="https://youtu.be/eca6kcFntiE">The Basics of Single Node Parallel Computing &#40;Lecture&#41;</a></p> <li><p><a href="/notes/05/">The Basics of Single Node Parallel Computing &#40;Notes&#41;</a></p> </ul> <h3 id=optional_extra_resources__5 ><a href="#optional_extra_resources__5" class=header-anchor >Optional Extra Resources</a></h3> <ul> <li><p><a href="http://ithare.com/wp-content/uploads/part101_infographics_v08.png">Chart of CPU Operation Costs</a></p> </ul> <p>Now that we have a concrete problem, let&#39;s start investigating ways to parallelize its solution. We will first see that many systems have an almost automatic way of parallelizing through array operations, which we will call array-based parallelism. The ability to easily parallelize large blocked linear algebra will be discussed, along with libraries like OpenBLAS, Intel MKL, CuBLAS &#40;GPU parallelism&#41; and Elemental.jl. This gives a form of Within-Method Parallelism which we can use to optimize specific algorithms which utilize linearity. Another form of parallelism is to parallelize over the inputs. We will describe how this is a form of data parallelism, and use this as a framework to introduce shared memory and distributed parallelism. The interactions between these parallelization methods and application considerations will be discussed.</p> <h2 id=lecture_6_styles_of_parallelism ><a href="#lecture_6_styles_of_parallelism" class=header-anchor >Lecture 6: Styles of Parallelism</a></h2> <ul> <li><p><a href="https://youtu.be/EP5VWwPIews">The Different Flavors of Parallelism: Parallel Programming Models &#40;Lecture&#41;</a></p> <li><p><a href="/notes/06/">The Different Flavors of Parallelism &#40;Notes&#41;</a></p> </ul> <p>Here we continue down the line of describing methods of parallelism by giving a high level overview of the types of parallelism. SIMD and multithreading are reviewed as the basic forms of parallelism where message passing is not a concern. Then accelerators, such as GPUs and TPUs are introduced. Moving further, distributed parallel computing and its models are showcased. What we will see is that what kind of parallelism we are doing actually is not the main determiner as to how we need to think about parallelism. Instead, the determining factor is the parallel programming model, where just a handful of models, like task-based parallelism or SPMD models, are seen across all of the different hardware abstractions.</p> <h2 id=lecture_7_ordinary_differential_equations_applications_and_discretizations ><a href="#lecture_7_ordinary_differential_equations_applications_and_discretizations" class=header-anchor >Lecture 7: Ordinary Differential Equations: Applications and Discretizations</a></h2> <ul> <li><p><a href="https://youtu.be/riAbPZy9gFc">Ordinary Differential Equations 1: Applications and Solution Characteristics &#40;Lecture&#41;</a></p> <li><p><a href="https://youtu.be/HMmOk9GIhsw">Ordinary Differential Equations 2: Discretizations and Stability &#40;Lecture&#41;</a></p> <li><p><a href="/notes/07/">Ordinary Differential Equations: Applications and Discretizations &#40;Notes&#41;</a></p> </ul> <p>In this lecture we will describe ordinary differential equations, where they arise in scientific contexts, and how they are solved. We will see that understanding the properties of the numerical methods requires understanding the dynamics of the discrete system generated from the approximation to the continuous system, and thus stability of a numerical method is directly tied to the stability properties of the dynamics. This gives the idea of stiffness, which is a larger computational idea about ill-conditioned systems.</p> <h2 id=lecture_8_forward-mode_automatic_differentiation ><a href="#lecture_8_forward-mode_automatic_differentiation" class=header-anchor >Lecture 8: Forward-Mode Automatic Differentiation</a></h2> <ul> <li><p><a href="https://youtu.be/zHPXGBiTM5A">Forward-Mode Automatic Differentiation &#40;AD&#41; via High Dimensional Algebras &#40;Lecture&#41;</a></p> <li><p><a href="/notes/08/">Forward-Mode Automatic Differentiation &#40;AD&#41; via High Dimensional Algebras &#40;Notes&#41;</a></p> </ul> <p>As we will soon see, the ability to calculate derivatives underpins a lot of problems in both scientific computing and machine learning. We will specifically see it show up in later lectures on solving implicit equations f&#40;x&#41;&#61;0 for stiff ordinary differential equation solvers, and in fitting neural networks. The common high performance way that this is done is called automatic differentiation. This lecture introduces the methods of forward and reverse mode automatic differentiation to setup future studies uses of the technique.</p> <h2 id=lecture_9_solving_stiff_ordinary_differential_equations ><a href="#lecture_9_solving_stiff_ordinary_differential_equations" class=header-anchor >Lecture 9: Solving Stiff Ordinary Differential Equations</a></h2> <h3 id=lecture_notes ><a href="#lecture_notes" class=header-anchor >Lecture Notes</a></h3> <ul> <li><p><a href="https://youtu.be/bY2VCoxMuo8">Solving Stiff Ordinary Differential Equations &#40;Lecture&#41;</a></p> <li><p><a href="/notes/09/">Solving Stiff Ordinary Differential Equations &#40;Notes&#41;</a></p> </ul> <h3 id=additional_readings_on_convergence_of_newtons_method ><a href="#additional_readings_on_convergence_of_newtons_method" class=header-anchor >Additional Readings on Convergence of Newton&#39;s Method</a></h3> <ul> <li><p><a href="https://link.springer.com/chapter/10.1007&#37;2F978-1-4612-0701-6_8">Newton&#39;s Method</a></p> <li><p><a href="https://pdfs.semanticscholar.org/1844/34b366f337972aa94a601fabd251d0baf62f.pdf">Relaxed Newton&#39;s Method</a></p> <li><p><a href="https://www.sciencedirect.com/science/article/pii/S00243795130067820">Convergence of Pure and Relaxed Newton Methods</a></p> <li><p><a href="http://cswiercz.info/2016/01/20/narc-talk.html">Smale&#39;s Alpha Theory for Newton Convergence</a></p> <li><p><a href="https://arxiv.org/abs/1011.1091">alphaCertified: certifying solutions to polynomial systems</a></p> <li><p><a href="https://arxiv.org/ftp/arxiv/papers/1503/1503.03543.pdf">Improved convergence theorem for Newton</a></p> <li><p><a href="https://www.math.uwaterloo.ca/~wgilbert/Research/GilbertFractals.pdf">Generalizations of Newton&#39;s Method</a></p> </ul> <p>Solving stiff ordinary differential equations, especially those which arise from partial differential equations, are the common bottleneck of scientific computing. The largest-scale scientific computing models are generally using heavy compute power in order to tackle some implicitly timestepped PDE solve&#33; Thus we will take a deep dive into how the different methods which are combined to create a stiff ordinary differential equation solver, looking at different aspects of Jacobian computations and linear solving and the effects that they have.</p> <h2 id=lecture_10_basic_parameter_estimation_reverse-mode_ad_and_inverse_problems ><a href="#lecture_10_basic_parameter_estimation_reverse-mode_ad_and_inverse_problems" class=header-anchor >Lecture 10: Basic Parameter Estimation, Reverse-Mode AD, and Inverse Problems</a></h2> <ul> <li><p><a href="https://youtu.be/XQAe4pEZ6L4">Basic Parameter Estimation, Reverse-Mode AD, and Inverse Problems &#40;Lecture&#41;</a></p> <li><p><a href="/notes/10/">Basic Parameter Estimation, Reverse-Mode AD, and Inverse Problems &#40;Notes&#41;</a></p> </ul> <p>Now that we have models, how do you fit the models to data? This lecture goes through the basic shooting method for parameter estimation, showcases how it&#39;s equivalent to training neural networks, and gives an in-depth discussion of how reverse-mode automatic differentiation is utilized in the training process for the efficient calculation of gradients.</p> <h2 id=lecture_11_differentiable_programming_and_neural_differential_equations ><a href="#lecture_11_differentiable_programming_and_neural_differential_equations" class=header-anchor >Lecture 11: Differentiable Programming and Neural Differential Equations</a></h2> <ul> <li><p><a href="https://youtu.be/fXcekZZP-1A">Differentiable Programming Part 1: Reverse-Mode AD Implementation &#40;Lecture&#41;</a></p> <li><p><a href="https://youtu.be/KCTfPyVIxpc">Differentiable Programming Part 2: Adjoint Derivation for &#40;Neural&#41; ODEs and Nonlinear Solve &#40;Lecture&#41;</a></p> <li><p><a href="/notes/11/">Differentiable Programming and Neural Differential Equations &#40;Notes&#41;</a></p> </ul> <h3 id=additional_readings_on_ad_implementations ><a href="#additional_readings_on_ad_implementations" class=header-anchor >Additional Readings on AD Implementations</a></h3> <ul> <li><p><a href="https://youtu.be/mQnSRfseu0c">Non-local compiler transformations in the presence of dynamic dispatch &#40;Diffractor.jl and higher order AD via Category Theory&#41;</a></p> <li><p><a href="https://youtu.be/BzuEGdGHKjc">JAX: accelerated machine learning research via composable function transformations in Python</a></p> </ul> <p>Given the efficiency of reverse-mode automatic differentiation, we want to see how far we can push this idea. How could one implement reverse-mode AD without computational graphs, and include problems like nonlinear solving and ordinary differential equations? Are there methods other than shooting methods that can be utilized for parameter fitting? This lecture will explore where reverse-mode AD intersects with scientific modeling, and where machine learning begins to enter scientific computing.</p> <h2 id=lecture_12_sciml_in_practice ><a href="#lecture_12_sciml_in_practice" class=header-anchor >Lecture 12: SciML in Practice</a></h2> <h3 id=lecture_121_mpi_for_distributed_computing ><a href="#lecture_121_mpi_for_distributed_computing" class=header-anchor >Lecture 12.1: MPI for Distributed Computing</a></h3> <p>Guest Lecturer: Lauren E. Milechin, MIT Lincoln Lab and the MIT Supercloud Guest Writer: Jeremy Kepner, MIT Lincoln Lab and the MIT Supercloud</p> <ul> <li><p><a href="https://www.youtube.com/watch?v&#61;LCIJj0czofo">Introduction to MPI.jl &#40;Lecture&#41;</a></p> <li><p><a href="/notes/12/">Introduction to MPI.jl &#40;Notes: PDF&#41;</a></p> </ul> <p>In this lecture we went over the basics of MPI &#40;Message Passing Interface&#41; for distributed computing and examples on how to use MPI.jl to write parallel programs that work efficiently over multiple computers &#40;or &quot;compute nodes&quot;&#41;. The MPI programming model and the job scripts required for using MPI on the MIT Supercloud HPC were demonstrated.</p> <h3 id=lecture_122_mathematics_of_machine_learning_and_high_performance_computing ><a href="#lecture_122_mathematics_of_machine_learning_and_high_performance_computing" class=header-anchor >Lecture 12.2: Mathematics of Machine Learning and High Performance Computing</a></h3> <p>Guest Lecturer: Jeremy Kepner, MIT Lincoln Lab and the MIT Supercloud</p> <ul> <li><p><a href="https://youtu.be/0sKPkJME2Jw?t&#61;26">Mathematics of Big Data and Machine Learning &#40;Lecture&#41;</a></p> <li><p><a href="https://youtu.be/gZSNp6XbOK8?t&#61;17">Mathematical Foundations of the GraphBLAS and Big Data &#40;Lecture&#41;</a></p> <li><p><a href="https://youtu.be/RpPlj2HnuWg?t&#61;1412">AI Data Architecture &#40;Lecture&#41;</a></p> <li><p><a href="https://github.com/mitmath/18337/blob/master/lecture12/PerformanceMetricsSoftwareArchitecture.pdf">Performance Metrics and Software Architecture &#40;Book Chapter&#41;</a></p> <li><p><a href="https://github.com/mitmath/18337/blob/master/lecture12/OptimizingXeonPhi-PID6086383.pdf">Optimizing Xeon Phi for Interactive Data Analysis &#40;Paper&#41;</a></p> </ul> <p>In this lecture we went over the mathematics behind big data, machine learning, and high performance computing. Pieces like Amdahl&#39;s law for describing maximal parallel compute efficiency were described and demonstrated to showcase some hard ceiling on the capabilities of parallel computing, and these laws were described in the context of big data computations in order to assess the viability of distributed computing within that domain&#39;s context.</p> <h2 id=lecture_13_gpu_computing ><a href="#lecture_13_gpu_computing" class=header-anchor >Lecture 13: GPU Computing</a></h2> <p>Guest Lecturer: Valentin Churavy, MIT Julia Lab</p> <ul> <li><p><a href="https://youtu.be/KCYlEub_8xc">Parallel Computing: From SIMD to SIMT &#40;Lecture&#41;</a></p> <li><p><a href="https://youtu.be/v9bFRg4rUfk">GPU Computing in Julia</a></p> <li><p><a href="/notes/13/">GPU Programming in Julia</a></p> <li><p><a href="https://docs.google.com/presentation/d/1C1dt8zeNW7spgswr2CmLrE0G-ayj0ItvoEWHdX_0kYc/edit#slide&#61;id.g76b4384d33_0_5">Parallel Computing: From SIMD to SIMT &#40;Notes&#41;</a></p> <li><p><a href="https://docs.google.com/presentation/d/1QvHE_xVDKnPA3-nowzpZY1lUdXr7B8rLCu2usOz8KT8/edit#slide&#61;id.gb00e54ec3a_0_477">GPU Computing in Julia &#40;Notes&#41;</a></p> </ul> <p>In this lecture we take a deeper dive into the architectural differences of GPUs and how that changes the parallel computing mindset that&#39;s required to arrive at efficient code. Valentin walks through the compilation process and how the resulting behaviors are due to core trade-offs in GPU-based programming and direct compilation for such hardware.</p> <h2 id=lecture_14_partial_differential_equations_and_convolutional_neural_networks ><a href="#lecture_14_partial_differential_equations_and_convolutional_neural_networks" class=header-anchor >Lecture 14: Partial Differential Equations and Convolutional Neural Networks</a></h2> <ul> <li><p><a href="https://youtu.be/apkyk8n0vBo">PDEs, Convolutions, and the Mathematics of Locality &#40;Lecture&#41;</a></p> <li><p><a href="/notes/14/">PDEs, Convolutions, and the Mathematics of Locality &#40;Notes&#41;</a></p> </ul> <h3 id=additional_readings ><a href="#additional_readings" class=header-anchor >Additional Readings</a></h3> <ul> <li><p><a href="https://arxiv.org/abs/1804.04272">Deep Neural Networks Motivated by Partial Differential Equations</a></p> </ul> <p>In this lecture we will continue to relate the methods of machine learning to those in scientific computing by looking at the relationship between convolutional neural networks and partial differential equations. It turns out they are more than just similar: the two are both stencil computations on spatial data&#33;</p> <h2 id=lecture_15_more_algorithms_which_connect_differential_equations_and_machine_learning ><a href="#lecture_15_more_algorithms_which_connect_differential_equations_and_machine_learning" class=header-anchor >Lecture 15: More Algorithms which Connect Differential Equations and Machine Learning</a></h2> <ul> <li><p><a href="https://youtu.be/YuaVXt--gAA">Mixing Differential Equations and Neural Networks for Physics-Informed Learning &#40;Lecture&#41;</a></p> <li><p><a href="/notes/15/">Mixing Differential Equations and Neural Networks for Physics-Informed Learning &#40;Notes&#41;</a></p> </ul> <p>Neural ordinary differential equations and physics-informed neural networks are only the tip of the iceberg. In this lecture we will look into other algorithms which are utilizing the connection between neural networks and machine learning. We will generalize to augmented neural ordinary differential equations and universal differential equations with DiffEqFlux.jl, which now allows for stiff equations, stochasticity, delays, constraint equations, event handling, etc. to all take place in a neural differential equation format. Then we will dig into the methods for solving high dimensional partial differential equations through transformations to backwards stochastic differential equations &#40;BSDEs&#41;, and the applications to mathematical finance through Black-Scholes along with stochastic optimal control through Hamilton-Jacobi-Bellman equations. We then look into alternative training techniques using reservoir computing, such as continuous-time echo state networks, which alleviate some of the gradient issues associated with training neural networks on stiff and chaotic dynamical systems. We showcase a few of the methods which are being used to automatically discover equations in their symbolic form such as SINDy. To end it, we look into methods for accelerating differential equation solving through neural surrogate models, and uncover the true idea of what&#39;s going on, along with understanding when these applications can be used effectively.</p> <h2 id=lecture_16_probabilistic_programming ><a href="#lecture_16_probabilistic_programming" class=header-anchor >Lecture 16: Probabilistic Programming</a></h2> <ul> <li><p><a href="https://youtu.be/32rAwtTAGdU">From Optimization to Probabilistic Programming &#40;Lecture&#41;</a></p> <li><p><a href="/notes/16/">From Optimization to Probabilistic Programming &#40;Notes&#41;</a></p> </ul> <p>All of our previous discussions lived in a deterministic world. Not this one. Here we turn to a probabilistic view and allow programs to have random variables. Forward simulation of a random program is seen to be simple through Monte Carlo sampling. However, parameter estimation is now much more involved, since in this case we need to estimate not just values but probability distributions. It turns out that Bayes&#39; rule gives a framework for performing such estimations. We see that classical parameter estimation falls out as a maximization of probability with the &quot;simplest&quot; form of distributions, and thus this gives a nice generalization even to standard parameter estimation and justifies the use of L2 loss functions and regularization &#40;as a perturbation by a prior&#41;. Next, we turn to estimating the distributions, which we see is possible for small problems using Metropolis Hastings, but for larger problems we develop Hamiltonian Monte Carlo. It turns out that Hamiltonian Monte Carlo has strong ties to both ODEs and differentiable programming: it is defined as solving ODEs which arise from a Hamiltonian, and derivatives of the likelihood are required, which is essentially the same idea as derivatives of cost functions&#33; We then describe an alternative approach: Automatic Differentiation Variational Inference &#40;ADVI&#41;, which once again is using the tools of differentiable programming to estimate distributions of probabilistic programs.</p> <h2 id=lecture_17_global_sensitivity_analysis ><a href="#lecture_17_global_sensitivity_analysis" class=header-anchor >Lecture 17: Global Sensitivity Analysis</a></h2> <ul> <li><p><a href="https://youtu.be/wzTpoINJyBQ">Global Sensitivity Analysis &#40;Lecture&#41;</a></p> <li><p><a href="/notes/17/">Global Sensitivity Analysis &#40;Notes&#41;</a></p> </ul> <p>Our previous analysis of sensitivities was all local. What does it mean to example the sensitivities of a model globally? It turns out the probabilistic programming viewpoint gives us a solid way of describing how we expect values to be changing over larger sets of parameters via the random variables that describe the program&#39;s inputs. This means we can decompose the output variance into indices which can be calculated via various quadrature approximations which then give a tractable measurement to &quot;variable x has no effect on the mean solution&quot;.</p> <h2 id=lecture_18_code_profiling_and_optimization ><a href="#lecture_18_code_profiling_and_optimization" class=header-anchor >Lecture 18: Code Profiling and Optimization</a></h2> <ul> <li><p><a href="https://youtu.be/h-xVBD2Pk9o">Code Profiling and Optimization &#40;Lecture&#41;</a></p> <li><p><a href="/notes/18/">Code Profiling and Optimization &#40;Notes&#41;</a></p> </ul> <p>How do you put everything together in this course? Let&#39;s take a look at a PDE solver code given in a method of lines form. In this lecture I walk through the code and demonstrate how to serial optimize it, and showcase the interaction between variable caching and automatic differentiation.</p> <h2 id=lecture_19_uncertainty_programming_and_generalized_uncertainty_quantification ><a href="#lecture_19_uncertainty_programming_and_generalized_uncertainty_quantification" class=header-anchor >Lecture 19: Uncertainty Programming and Generalized Uncertainty Quantification</a></h2> <ul> <li><p><a href="https://youtu.be/MRTXK2Vc0YE">Uncertainty Programming &#40;Lecture&#41;</a></p> <li><p><a href="/notes/19/">Uncertainty Programming &#40;Notes&#41;</a></p> </ul> <p>We end the course by taking a look at another mathematical topic to see whether it can be addressed in a similar manner: uncertainty quantification &#40;UQ&#41;. There are ways which it can be handled similar to automatic differentiation. Measurements.jl gives a forward-propagation approach, somewhat like ForwardDiff&#39;s dual numbers, through a number type which is representative of normal distributions and pushes these values through a program. This has many advantages, since it allows for uncertainty quantification without sampling, but turns the number types into a value that is heap allocated. Other approaches are investigated, like interval arithmetic which is rigorous but limited in scope. But on the entirely other end, a non-general method for ODEs is shown which utilizes the trajectory structure of the differential equation solution and doesn&#39;t give the blow up that the other methods see. This showcases that uses higher level information can be helpful in UQ, and less local approaches may be necessary. We end by showcasing the Koopman operator as the adjoint of the pushforward of the uncertainty measure, and as an adjoint method it can give accelerated computations of uncertainty against cost functions.</p> <h2 id=final_project ><a href="#final_project" class=header-anchor >Final Project</a></h2> <p>The final project is a 10-20 page paper using the style template from the <a href="http://www.siam.org/journals/auth-info.php"><em>SIAM Journal on Numerical Analysis</em></a> &#40;or similar&#41;. The final project must include code for a high performance &#40;or parallelized&#41; implementation of the algorithm in a form that is usable by others. A thorough performance analysis is expected. Model your paper on academic review articles &#40;e.g. read <em>SIAM Review</em> and similar journals for examples&#41;.</p> <p>One possibility is to review an interesting algorithm not covered in the course and develop a high performance implementation. Some examples include:</p> <ul> <li><p>High performance PDE solvers for specific PDEs like Navier-Stokes</p> <li><p>Common high performance algorithms &#40;Ex: Jacobian-Free Newton Krylov for PDEs&#41;</p> <li><p>Recreation of a parameter sensitivity study in a field like biology, pharmacology, or climate science</p> <li><p><a href="https://arxiv.org/abs/1904.01681">Augmented Neural Ordinary Differential Equations</a></p> <li><p><a href="https://arxiv.org/pdf/1905.10403.pdf">Neural Jump Stochastic Differential Equations</a></p> <li><p>Parallelized stencil calculations</p> <li><p>Distributed linear algebra kernels</p> <li><p>Parallel implementations of statistical libraries, such as survival statistics or linear models for big data. Here&#39;s <a href="https://github.com/harrelfe/rms">one example parallel library&#41;</a> and a <a href="https://bioconductor.org/packages/release/data/experiment/html/RegParallel.html">second example</a>.</p> <li><p>Parallelization of data analysis methods</p> <li><p>Type-generic implementations of sparse linear algebra methods</p> <li><p>A fast regex library</p> <li><p>Math library primitives &#40;exp, log, etc.&#41;</p> </ul> <p>Another possibility is to work on state-of-the-art performance engineering. This would be implementing a new auto-parallelization or performance enhancement. For these types of projects, implementing an application for benchmarking is not required, and one can instead benchmark the effects on already existing code to find cases where it is beneficial &#40;or leads to performance regressions&#41;. Possible examples are:</p> <ul> <li><p><a href="https://github.com/JuliaLang/julia/issues/19777">Create a system for automatic multithreaded parallelism of array operations</a> and see what kinds of packages end up more efficient</p> <li><p><a href="https://github.com/JuliaLang/julia/issues/32786">Setup BLAS with a PARTR backend</a> and investigate the downstream effects on multithreaded code like an existing PDE solver</p> <li><p><a href="https://github.com/JuliaLang/julia/issues/21017">Investigate the effects of work-stealing in multithreaded loops</a></p> <li><p>Fast parallelized type-generic FFT. Starter code by Steven Johnson &#40;creator of FFTW&#41; and Yingbo Ma <a href="https://github.com/YingboMa/DFT.jl">can be found here</a></p> <li><p>Type-generic BLAS. <a href="https://github.com/JuliaBLAS/JuliaBLAS.jl">Starter code can be found here</a></p> <li><p>Implementation of parallelized map-reduce methods. For example, <code>pmapreduce</code> <a href="https://docs.julialang.org/en/v1/manual/parallel-computing/index.html">extension to <code>pmap</code></a> that adds a paralellized reduction, or a fast GPU-based map-reduce.</p> <li><p>Investigating auto-compilation of full package codes to GPUs using tools like <a href="https://github.com/JuliaGPU/CUDAnative.jl">CUDAnative</a> and/or <a href="https://github.com/vchuravy/GPUifyLoops.jl">GPUifyLoops</a>.</p> <li><p>Investigating alternative implementations of databases and dataframes. <a href="https://github.com/JuliaData/DataFrames.jl/issues/1335">NamedTuple backends of DataFrames</a>, alternative <a href="https://github.com/FugroRoames/TypedTables.jl">type-stable DataFrames</a>, defaults for CSV reading and other large-table formats like <a href="https://github.com/JuliaComputing/JuliaDB.jl">JuliaDB</a>.</p> </ul> <p>Additionally, Scientific Machine Learning is a wide open field with lots of low hanging fruit. Instead of a review, a suitable research project can be used for chosen for the final project. Possibilities include:</p> <ul> <li><p>Acceleration methods for adjoints of differential equations</p> <li><p>Improved methods for Physics-Informed Neural Networks</p> <li><p>New applications of neural differential equations</p> <li><p>Parallelized implicit ODE solvers for large ODE systems</p> <li><p>GPU-parallelized ODE/SDE solvers for small systems</p> </ul> <p>Final project topics must be declared by October 30th with a 1 page extended abstract.</p> <div class=back-to-top > <span><a href="#" title="Back to Top"><i class="fa fa-chevron-circle-up"></i></a></span> </div> <div class=page-foot > <div class=copyright > <a href=https://github.com/SciML/SciMLBook>SciML Book source code</a> <br> &copy; Chris Rackauckas. Last modified: March 08, 2022. <br> Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> </div> </div>