
<h1 class="title">Differentiable Programming and Neural Differential Equations</h1>
<h5>Chris Rackauckas</h5>
<h5>November 20th, 2020</h5>

<h2><a href="https://youtu.be/fXcekZZP-1A">Youtube Video Link</a></h2>
<p>Our last discussion focused on how, at a high mathematical level, one could in theory build programs which compute gradients in a fast manner by looking at the computational graph and performing reverse-mode automatic differentiation. Within the context of parameter identification, we saw many advantages to this approach because it did not scale multiplicatively in the number of parameters, and thus it is an efficient way to calculate Jacobians of objects where there are less rows than columns &#40;think of the gradient as 1 row&#41;.</p>
<p>More precisely, this is seen to be more about sparsity patterns, with reverse-mode as being more efficient if there are &quot;enough&quot; less row seeds required than column partials &#40;with mixed mode approaches sometimes being much better&#41;. However, to make reverse-mode AD realistically usable inside of a programming language instead of a compute graph, we need to do three things:</p>
<ol>
<li><p>We need to have a way of implementing reverse-mode AD on a language.</p>
</li>
<li><p>We need a systematic way to derive &quot;adjoint&quot; relationships &#40;pullbacks&#41;.</p>
</li>
<li><p>We need to see if there are better ways to fit parameters to data, rather than performing reverse-mode AD through entire programs&#33;</p>
</li>
</ol>
<h2>Implementation of Reverse-Mode AD</h2>
<p>Forward-mode AD was implementable through operator overloading and dual number arithmetic. However, reverse-mode AD requires reversing a program through its computational structure, which is a much more difficult operation. This begs the question, how does one actually build a reverse-mode AD implementation?</p>
<h3>Static Graph AD</h3>
<p>The most obvious solution is to use a static compute graph since how we defined our differentiation structure was on a compute graph. Tensorflow is a modern example of this approach, where a user must define variables and operations in a graph language &#40;that&#39;s embedded into Python, R, Julia, etc.&#41;, and then execution on the graph is easy to differentiate. This has the advantage of being a simplified and controlled form, which means that not only differentiation transformations are possible, but also things like automatic parallelization. However, many see directly writing a &#40;static&#41; computation graph as a barrier for practical use since it requires completely rewriting all existing programs to this structure.</p>
<h3>Tracing-Based AD and Wengert Lists</h3>
<p>Recall that an alternative formulation of reverse-mode AD for composed functions</p>
<p class="math">\[
f = f^L \circ f^{L-1} \circ \ldots \circ f^1
\]</p>
<p>is through pullbacks on the Jacobians:</p>
<p class="math">\[
v^T J = (\ldots ((v^T J_L) J_{L-1}) \ldots ) J_1
\]</p>
<p>Therefore, if one can transform the program structure into a list of composed functions, then reverse-mode AD is the successive application of pullbacks going in the reverse direction:</p>
<p class="math">\[
\mathcal{B}_{f}^{x}(A)=\mathcal{B}_{f^{1}}^{x}\left(\ldots\left(\mathcal{\mathcal{B}}_{f^{L-1}}^{f^{L-2}(f^{L-3}(\ldots f^{1}(x)\ldots))}\left(\mathcal{B}_{f^{L}}^{f^{L-1}(f^{L-2}(\ldots f^{1}(x)\ldots))}(A)\right)\right)\ldots\right)
\]</p>
<p>Recall that the pullback <span class="math">$\mathcal{B}_f^x(\overline{y})$</span> requires knowing:</p>
<ol>
<li><p>The operation being performed</p>
</li>
<li><p>The value <span class="math">$x$</span> of the forward pass</p>
</li>
</ol>
<p>The idea is to then build a <em>Wengert list</em> that is from exactly the forward pass of a specific <span class="math">$x$</span>, also known as a <em>trace</em>, and thus giving rise to <em>tracing-based reverse-mode AD</em>. This is the basis of many reverse-mode implementations, such as Julia&#39;s Tracker.jl &#40;an old AD system used in ML&#41;, ReverseDiff.jl, PyTorch, Tensorflow Eager, Autograd, and Autograd.jl. It is widely adopted due to its simplicity in implementation.</p>
<h4>Inspecting Tracker.jl</h4>
<p>Tracker.jl is a very simple implementation to inspect. The definition of its number and array types are as follows:</p>


<pre class='hljl'>
<span class='hljl-k'>struct</span><span class='hljl-t'> </span><span class='hljl-nf'>Call</span><span class='hljl-p'>{</span><span class='hljl-n'>F</span><span class='hljl-p'>,</span><span class='hljl-n'>As</span><span class='hljl-oB'>&lt;:</span><span class='hljl-n'>Tuple</span><span class='hljl-p'>}</span><span class='hljl-t'>
  </span><span class='hljl-n'>func</span><span class='hljl-oB'>::</span><span class='hljl-n'>F</span><span class='hljl-t'>
  </span><span class='hljl-n'>args</span><span class='hljl-oB'>::</span><span class='hljl-n'>As</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-t'>

</span><span class='hljl-k'>mutable struct</span><span class='hljl-t'> </span><span class='hljl-nf'>Tracked</span><span class='hljl-p'>{</span><span class='hljl-n'>T</span><span class='hljl-p'>}</span><span class='hljl-t'>
  </span><span class='hljl-n'>ref</span><span class='hljl-oB'>::</span><span class='hljl-n'>UInt32</span><span class='hljl-t'>
  </span><span class='hljl-n'>f</span><span class='hljl-oB'>::</span><span class='hljl-n'>Call</span><span class='hljl-t'>
  </span><span class='hljl-n'>isleaf</span><span class='hljl-oB'>::</span><span class='hljl-n'>Bool</span><span class='hljl-t'>
  </span><span class='hljl-n'>grad</span><span class='hljl-oB'>::</span><span class='hljl-n'>T</span><span class='hljl-t'>
  </span><span class='hljl-nf'>Tracked</span><span class='hljl-p'>{</span><span class='hljl-n'>T</span><span class='hljl-p'>}(</span><span class='hljl-n'>f</span><span class='hljl-oB'>::</span><span class='hljl-n'>Call</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-kp'>where</span><span class='hljl-t'> </span><span class='hljl-n'>T</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>new</span><span class='hljl-p'>(</span><span class='hljl-ni'>0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>f</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-kc'>false</span><span class='hljl-p'>)</span><span class='hljl-t'>
  </span><span class='hljl-nf'>Tracked</span><span class='hljl-p'>{</span><span class='hljl-n'>T</span><span class='hljl-p'>}(</span><span class='hljl-n'>f</span><span class='hljl-oB'>::</span><span class='hljl-n'>Call</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>grad</span><span class='hljl-oB'>::</span><span class='hljl-n'>T</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-kp'>where</span><span class='hljl-t'> </span><span class='hljl-n'>T</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>new</span><span class='hljl-p'>(</span><span class='hljl-ni'>0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>f</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-kc'>false</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>grad</span><span class='hljl-p'>)</span><span class='hljl-t'>
  </span><span class='hljl-nf'>Tracked</span><span class='hljl-p'>{</span><span class='hljl-n'>T</span><span class='hljl-p'>}(</span><span class='hljl-n'>f</span><span class='hljl-oB'>::</span><span class='hljl-nf'>Call</span><span class='hljl-p'>{</span><span class='hljl-n'>Nothing</span><span class='hljl-p'>},</span><span class='hljl-t'> </span><span class='hljl-n'>grad</span><span class='hljl-oB'>::</span><span class='hljl-n'>T</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-kp'>where</span><span class='hljl-t'> </span><span class='hljl-n'>T</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>new</span><span class='hljl-p'>(</span><span class='hljl-ni'>0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>f</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-kc'>true</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>grad</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-t'>

</span><span class='hljl-k'>mutable struct</span><span class='hljl-t'> </span><span class='hljl-nf'>TrackedReal</span><span class='hljl-p'>{</span><span class='hljl-n'>T</span><span class='hljl-oB'>&lt;:</span><span class='hljl-n'>Real</span><span class='hljl-p'>}</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;:</span><span class='hljl-t'> </span><span class='hljl-n'>Real</span><span class='hljl-t'>
  </span><span class='hljl-n'>data</span><span class='hljl-oB'>::</span><span class='hljl-n'>T</span><span class='hljl-t'>
  </span><span class='hljl-n'>tracker</span><span class='hljl-oB'>::</span><span class='hljl-nf'>Tracked</span><span class='hljl-p'>{</span><span class='hljl-n'>T</span><span class='hljl-p'>}</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-t'>

</span><span class='hljl-k'>struct</span><span class='hljl-t'> </span><span class='hljl-nf'>TrackedArray</span><span class='hljl-p'>{</span><span class='hljl-n'>T</span><span class='hljl-p'>,</span><span class='hljl-n'>N</span><span class='hljl-p'>,</span><span class='hljl-n'>A</span><span class='hljl-oB'>&lt;:</span><span class='hljl-nf'>AbstractArray</span><span class='hljl-p'>{</span><span class='hljl-n'>T</span><span class='hljl-p'>,</span><span class='hljl-n'>N</span><span class='hljl-p'>}}</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;:</span><span class='hljl-t'> </span><span class='hljl-nf'>AbstractArray</span><span class='hljl-p'>{</span><span class='hljl-n'>T</span><span class='hljl-p'>,</span><span class='hljl-n'>N</span><span class='hljl-p'>}</span><span class='hljl-t'>
  </span><span class='hljl-n'>tracker</span><span class='hljl-oB'>::</span><span class='hljl-nf'>Tracked</span><span class='hljl-p'>{</span><span class='hljl-n'>A</span><span class='hljl-p'>}</span><span class='hljl-t'>
  </span><span class='hljl-n'>data</span><span class='hljl-oB'>::</span><span class='hljl-n'>A</span><span class='hljl-t'>
  </span><span class='hljl-n'>grad</span><span class='hljl-oB'>::</span><span class='hljl-n'>A</span><span class='hljl-t'>
  </span><span class='hljl-nf'>TrackedArray</span><span class='hljl-p'>{</span><span class='hljl-n'>T</span><span class='hljl-p'>,</span><span class='hljl-n'>N</span><span class='hljl-p'>,</span><span class='hljl-n'>A</span><span class='hljl-p'>}(</span><span class='hljl-n'>t</span><span class='hljl-oB'>::</span><span class='hljl-nf'>Tracked</span><span class='hljl-p'>{</span><span class='hljl-n'>A</span><span class='hljl-p'>},</span><span class='hljl-t'> </span><span class='hljl-n'>data</span><span class='hljl-oB'>::</span><span class='hljl-n'>A</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-kp'>where</span><span class='hljl-t'> </span><span class='hljl-p'>{</span><span class='hljl-n'>T</span><span class='hljl-p'>,</span><span class='hljl-n'>N</span><span class='hljl-p'>,</span><span class='hljl-n'>A</span><span class='hljl-p'>}</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>new</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>data</span><span class='hljl-p'>)</span><span class='hljl-t'>
  </span><span class='hljl-nf'>TrackedArray</span><span class='hljl-p'>{</span><span class='hljl-n'>T</span><span class='hljl-p'>,</span><span class='hljl-n'>N</span><span class='hljl-p'>,</span><span class='hljl-n'>A</span><span class='hljl-p'>}(</span><span class='hljl-n'>t</span><span class='hljl-oB'>::</span><span class='hljl-nf'>Tracked</span><span class='hljl-p'>{</span><span class='hljl-n'>A</span><span class='hljl-p'>},</span><span class='hljl-t'> </span><span class='hljl-n'>data</span><span class='hljl-oB'>::</span><span class='hljl-n'>A</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>grad</span><span class='hljl-oB'>::</span><span class='hljl-n'>A</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-kp'>where</span><span class='hljl-t'> </span><span class='hljl-p'>{</span><span class='hljl-n'>T</span><span class='hljl-p'>,</span><span class='hljl-n'>N</span><span class='hljl-p'>,</span><span class='hljl-n'>A</span><span class='hljl-p'>}</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>new</span><span class='hljl-p'>(</span><span class='hljl-n'>t</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>data</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>grad</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<p>As expected, it replaces every single number and array with a value that will store not just perform the operation, but also build up a list of operations along with the values at every stage. Then pullback rules are implemented for primitives via the <code>@grad</code> macro. For example, the pullback for the dot product is implemented as:</p>


<pre class='hljl'>
<span class='hljl-nd'>@grad</span><span class='hljl-t'> </span><span class='hljl-nf'>dot</span><span class='hljl-p'>(</span><span class='hljl-n'>xs</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ys</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>dot</span><span class='hljl-p'>(</span><span class='hljl-nf'>data</span><span class='hljl-p'>(</span><span class='hljl-n'>xs</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nf'>data</span><span class='hljl-p'>(</span><span class='hljl-n'>ys</span><span class='hljl-p'>)),</span><span class='hljl-t'> </span><span class='hljl-n'>Δ</span><span class='hljl-t'> </span><span class='hljl-oB'>-&gt;</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>Δ</span><span class='hljl-t'> </span><span class='hljl-oB'>.*</span><span class='hljl-t'> </span><span class='hljl-n'>ys</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>Δ</span><span class='hljl-t'> </span><span class='hljl-oB'>.*</span><span class='hljl-t'> </span><span class='hljl-n'>xs</span><span class='hljl-p'>)</span>
</pre>


<p>This is read as: the value going forward is computed by using the Julia <code>dot</code> function on the arrays, and the pullback embeds the backs of the forward pass and uses <code>Δ .* ys</code> as the derivative with respect to <code>x</code>, and <code>Δ .* xs</code> as the derivative with respect to <code>y</code>. This element-wise nature makes sense given the diagonal-ness of the Jacobian.</p>
<p>Note that this also allows utilizing intermediates of the forward pass within the reverse pass. This is seen in the definition of the pullback of <code>meanpool</code>:</p>


<pre class='hljl'>
<span class='hljl-nd'>@grad</span><span class='hljl-t'> </span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>meanpool</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>pdims</span><span class='hljl-oB'>::</span><span class='hljl-n'>PoolDims</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>kw</span><span class='hljl-oB'>...</span><span class='hljl-p'>)</span><span class='hljl-t'>
  </span><span class='hljl-n'>y</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>meanpool</span><span class='hljl-p'>(</span><span class='hljl-nf'>data</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-n'>pdims</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>kw</span><span class='hljl-oB'>...</span><span class='hljl-p'>)</span><span class='hljl-t'>
  </span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>Δ</span><span class='hljl-t'> </span><span class='hljl-oB'>-&gt;</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-nf'>nobacksies</span><span class='hljl-p'>(</span><span class='hljl-sc'>:meanpool</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>NNlib</span><span class='hljl-oB'>.</span><span class='hljl-nf'>∇meanpool</span><span class='hljl-p'>(</span><span class='hljl-n'>data</span><span class='hljl-oB'>.</span><span class='hljl-p'>((</span><span class='hljl-n'>Δ</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-p'>))</span><span class='hljl-oB'>...</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>pdims</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>kw</span><span class='hljl-oB'>...</span><span class='hljl-p'>)),</span><span class='hljl-t'> </span><span class='hljl-n'>nothing</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<p>where the derivative makes use of not only <code>x</code>, but also <code>y</code> so that the <code>meanpool</code> does not need to be re-calculated.</p>
<p>Using this style, Tracker.jl moves forward, building up the value and closures for the backpass and then recursively pulls back the input <code>Δ</code> to receive the derivative.</p>
<h3>Source-to-Source AD</h3>
<p>Given our previous discussions on performance, you should be horrified with how this approach handles scalar values. Each <code>TrackedReal</code> holds as <code>Tracked&#123;T&#125;</code> which holds a <code>Call</code>, not a <code>Call&#123;F,As&lt;:Tuple&#125;</code>, and thus it&#39;s not strictly typed. Because it&#39;s not strictly typed, this implies that every single operation is going to cause heap allocations. If you measure this in PyTorch, TensorFlow Eager, Tracker, etc. you get around 500ns-2ms of overhead. This means that a 2ns <code>&#43;</code> operation becomes... &gt;500ns&#33; Oh my&#33;</p>
<p>This is not the only issue with tracing. Another issue is that the trace is value-dependent, meaning that every new value can build a new trace. Thus one cannot easily JIT compile a trace because it&#39;ll be different for every gradient calculation &#40;you can compile it, but you better make sure the compile times are short&#33;&#41;. Lastly, the Wengert list can be much larger than the code itself. For example, if you trace through a loop that is <code>for i in 1:100000</code>, then the trace will be huge, even if the function is relatively simple. This is directly demonstrated in the JAX &quot;how it works&quot; slide:</p>
<p><img src="https://iaml.it/blog/jax-intro-english/images/lifecycle.png" alt="" /></p>
<p>To avoid these issues, another version of reverse-mode automatic differentiation is <em>source-to-source</em> transformations. In order to do source code transformations, you need to know how to transform all language constructs via the reverse pass. This can be quite difficult &#40;what is the &quot;adjoint&quot; of <code>lock</code>?&#41;, but when worked out this has a few benefits. First of all, you do not have to track values, meaning stack-allocated values can stay on the stack. Additionally, you can JIT compile one backpass because you have a single function used for all backpasses. Lastly, you don&#39;t need to unroll your loops&#33; Instead, which each branch you&#39;d need to insert some data structure to recall the values used from the forward pass &#40;in order to invert in the right directions&#41;. However, that can be much more lightweight than a tracking pass.</p>
<p>This can be a difficult problem to do in a general programming language. In general it needs a strong programmatic representation to use as a compute graph. Google&#39;s engineers did an analysis <a href="https://github.com/tensorflow/swift/blob/master/docs/WhySwiftForTensorFlow.md">when choosing Swift for TensorFlow</a> and narrowed it down to either Swift or Julia due to their internal graph structures. Thus, it should be no surprise that the modern source-to-source AD systems are Zygote.jl for Julia, and Swift for TensorFlow in Swift. Additionally, older AD systems, like Tampenade, ADIFOR, and TAF, all for Fortran, were source-to-source AD systems.</p>
<h3>Worked Example: Reverse-Mode AD on the Babylonian Square Root</h3>
<p>To make the above discussion concrete, let&#39;s work through every AD strategy on a single function: the Babylonian method for computing <span class="math">$\sqrt{x}$</span>.</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>f</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-t'>
    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>300</span><span class='hljl-t'>
        </span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.5</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-oB'>/</span><span class='hljl-n'>a</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
    </span><span class='hljl-n'>a</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
f &#40;generic function with 3 methods&#41;
</pre>


<p>This iteratively computes <span class="math">$\sqrt{x}$</span> via the recurrence <span class="math">$a_{n+1} = \frac{1}{2}(a_n + x/a_n)$</span>. After 300 iterations starting from <span class="math">$a_0 = x$</span>, the result converges to machine precision. Since <span class="math">$f(x) = \sqrt{x}$</span>, the exact derivative is <span class="math">$f'(x) = \frac{1}{2\sqrt{x}}$</span>.</p>
<h4>Step 1: Lowering to a Three-Address Form</h4>
<p>Before differentiating, we decompose each compound expression into elementary operations, each assigned to a temporary. This is the form that a compiler &#40;or an AD system&#41; actually sees:</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>f_lowered</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-t'>
    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>300</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp1</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>a</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp2</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-n'>tmp1</span><span class='hljl-t'>
        </span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.5</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-n'>tmp2</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
    </span><span class='hljl-n'>y</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>a</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
f_lowered &#40;generic function with 1 method&#41;
</pre>


<p>Each line is now a single primitive operation &#40;<code>/</code>, <code>&#43;</code>, <code>*</code>&#41; whose derivative rule we know. This decomposition is the starting point for all AD strategies.</p>
<h4>Step 2: Forward-Mode AD</h4>
<p>In forward-mode AD, we propagate a <em>tangent</em> <span class="math">$\dot{v}$</span> alongside each value <span class="math">$v$</span>. For each elementary operation, we apply the standard differentiation rule:</p>
<ul>
<li><p class="math">\[
\text{tmp1} = x/a \implies \dot{\text{tmp1}} = (\dot{x} \cdot a - \dot{a} \cdot x)/a^2
\]</p>
<p>&#40;quotient rule&#41;</p>
</li>
<li><p class="math">\[
\text{tmp2} = a + \text{tmp1} \implies \dot{\text{tmp2}} = \dot{a} + \dot{\text{tmp1}}
\]</p>
<p>&#40;sum rule&#41;</p>
</li>
<li><p class="math">\[
a = 0.5 \cdot \text{tmp2} \implies \dot{a} = 0.5 \cdot \dot{\text{tmp2}}
\]</p>
<p>&#40;constant scaling&#41;</p>
</li>
</ul>
<p>Seeding with <span class="math">$\dot{x} = 1$</span> gives us <span class="math">$\dot{y} = f'(x)$</span> at the output:</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>f_forward</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>dx</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>a</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>da</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>dx</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>300</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp1</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>dtmp1</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>a</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>dx</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-n'>da</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>a</span><span class='hljl-oB'>^</span><span class='hljl-ni'>2</span><span class='hljl-p'>)</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp2</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>dtmp2</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-n'>tmp1</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>da</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-n'>dtmp1</span><span class='hljl-p'>)</span><span class='hljl-t'>
        </span><span class='hljl-n'>a</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>da</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-nfB'>0.5</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-n'>tmp2</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.5</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-n'>dtmp2</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
    </span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>dy</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>a</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>da</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
f_forward &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-nf'>f_forward</span><span class='hljl-p'>(</span><span class='hljl-nfB'>2.0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nfB'>1.0</span><span class='hljl-p'>)</span><span class='hljl-t'>  </span><span class='hljl-cs'># (sqrt(2), 1/(2*sqrt(2)))</span>
</pre>


<pre class="output">
&#40;1.414213562373095, 0.35355339059327373&#41;
</pre>


<p>Forward-mode computes the derivative in a single forward sweep. Its cost scales with the number of <em>input</em> perturbation directions &#40;one pass per seed&#41;, so it&#39;s efficient when there are few inputs and many outputs.</p>
<h4>Step 3: Reverse-Mode AD — Dynamic Graph &#40;Tape-Based&#41;</h4>
<p>This is the approach used by PyTorch, ReverseDiff.jl, and Tracker.jl. During the forward pass, we record a <em>tape</em> &#40;Wengert list&#41; of every operation performed, along with the node IDs of inputs and outputs. Each intermediate value gets a unique node ID. Then the reverse pass walks the tape backward, applying pullback rules and accumulating adjoints:</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>apply_pullback</span><span class='hljl-p'>(</span><span class='hljl-oB'>::</span><span class='hljl-nf'>typeof</span><span class='hljl-p'>(</span><span class='hljl-n'>identity</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-n'>args</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>out</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>outbar</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-p'>(</span><span class='hljl-n'>outbar</span><span class='hljl-p'>,)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-t'>

</span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>apply_pullback</span><span class='hljl-p'>(</span><span class='hljl-oB'>::</span><span class='hljl-nf'>typeof</span><span class='hljl-p'>(</span><span class='hljl-oB'>/</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-n'>args</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>out</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>outbar</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>a</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>b</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>args</span><span class='hljl-t'>
    </span><span class='hljl-p'>(</span><span class='hljl-n'>outbar</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>b</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-n'>outbar</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>b</span><span class='hljl-oB'>^</span><span class='hljl-ni'>2</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-t'>

</span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>apply_pullback</span><span class='hljl-p'>(</span><span class='hljl-oB'>::</span><span class='hljl-nf'>typeof</span><span class='hljl-p'>(</span><span class='hljl-oB'>+</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-n'>args</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>out</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>outbar</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-p'>(</span><span class='hljl-n'>outbar</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>outbar</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-t'>

</span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>apply_pullback</span><span class='hljl-p'>(</span><span class='hljl-oB'>::</span><span class='hljl-nf'>typeof</span><span class='hljl-p'>(</span><span class='hljl-oB'>*</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-n'>args</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>out</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>outbar</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>a</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>b</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>args</span><span class='hljl-t'>
    </span><span class='hljl-p'>(</span><span class='hljl-n'>outbar</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-n'>b</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>outbar</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-n'>a</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
apply_pullback &#40;generic function with 4 methods&#41;
</pre>


<p>The pullback of each primitive is just the transpose of its Jacobian applied to the incoming adjoint <span class="math">$\bar{v}$</span>. For example, the pullback of <span class="math">$y = a/b$</span> gives <span class="math">$\bar{a} = \bar{y}/b$</span> and <span class="math">$\bar{b} = -\bar{y} \cdot a/b^2$</span>, which are exactly the entries of <span class="math">$J^T \bar{y}$</span>.</p>
<p>The forward pass builds the tape by assigning node IDs:</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>f_reverse_dynamic_ad</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>node_id</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>0</span><span class='hljl-t'>
    </span><span class='hljl-nf'>next_id</span><span class='hljl-p'>()</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>node_id</span><span class='hljl-t'> </span><span class='hljl-oB'>+=</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>node_id</span><span class='hljl-p'>)</span><span class='hljl-t'>

    </span><span class='hljl-n'>tape</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>[]</span><span class='hljl-t'>  </span><span class='hljl-cs'># entries: (op, input_ids, output_id)</span><span class='hljl-t'>
    </span><span class='hljl-n'>is_constant</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>Set</span><span class='hljl-p'>{</span><span class='hljl-n'>Int</span><span class='hljl-p'>}()</span><span class='hljl-t'>

    </span><span class='hljl-n'>x_id</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>next_id</span><span class='hljl-p'>()</span><span class='hljl-t'>

    </span><span class='hljl-n'>a_id</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>next_id</span><span class='hljl-p'>()</span><span class='hljl-t'>
    </span><span class='hljl-nf'>push!</span><span class='hljl-p'>(</span><span class='hljl-n'>tape</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>identity</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>x_id</span><span class='hljl-p'>,),</span><span class='hljl-t'> </span><span class='hljl-n'>a_id</span><span class='hljl-p'>))</span><span class='hljl-t'>

    </span><span class='hljl-n'>const_05_id</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>next_id</span><span class='hljl-p'>()</span><span class='hljl-t'>
    </span><span class='hljl-nf'>push!</span><span class='hljl-p'>(</span><span class='hljl-n'>is_constant</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>const_05_id</span><span class='hljl-p'>)</span><span class='hljl-t'>

    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>300</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp1_id</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>next_id</span><span class='hljl-p'>()</span><span class='hljl-t'>
        </span><span class='hljl-nf'>push!</span><span class='hljl-p'>(</span><span class='hljl-n'>tape</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-oB'>/</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>x_id</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>a_id</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-n'>tmp1_id</span><span class='hljl-p'>))</span><span class='hljl-t'>

        </span><span class='hljl-n'>tmp2_id</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>next_id</span><span class='hljl-p'>()</span><span class='hljl-t'>
        </span><span class='hljl-nf'>push!</span><span class='hljl-p'>(</span><span class='hljl-n'>tape</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-oB'>+</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>a_id</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>tmp1_id</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-n'>tmp2_id</span><span class='hljl-p'>))</span><span class='hljl-t'>

        </span><span class='hljl-n'>a_id</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>next_id</span><span class='hljl-p'>()</span><span class='hljl-t'>
        </span><span class='hljl-nf'>push!</span><span class='hljl-p'>(</span><span class='hljl-n'>tape</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-oB'>*</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>const_05_id</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>tmp2_id</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-n'>a_id</span><span class='hljl-p'>))</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>

    </span><span class='hljl-n'>y_id</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>a_id</span><span class='hljl-t'>

    </span><span class='hljl-cs'># Replay tape to get forward values</span><span class='hljl-t'>
    </span><span class='hljl-n'>node_vals</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>Dict</span><span class='hljl-p'>{</span><span class='hljl-n'>Int</span><span class='hljl-p'>,</span><span class='hljl-n'>Float64</span><span class='hljl-p'>}()</span><span class='hljl-t'>
    </span><span class='hljl-n'>node_vals</span><span class='hljl-p'>[</span><span class='hljl-n'>x_id</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-t'>
    </span><span class='hljl-n'>node_vals</span><span class='hljl-p'>[</span><span class='hljl-n'>const_05_id</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.5</span><span class='hljl-t'>

    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>op</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>in_ids</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>out_id</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-n'>tape</span><span class='hljl-t'>
        </span><span class='hljl-n'>args</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>ntuple</span><span class='hljl-p'>(</span><span class='hljl-n'>j</span><span class='hljl-t'> </span><span class='hljl-oB'>-&gt;</span><span class='hljl-t'> </span><span class='hljl-n'>node_vals</span><span class='hljl-p'>[</span><span class='hljl-n'>in_ids</span><span class='hljl-p'>[</span><span class='hljl-n'>j</span><span class='hljl-p'>]],</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>in_ids</span><span class='hljl-p'>))</span><span class='hljl-t'>
        </span><span class='hljl-n'>node_vals</span><span class='hljl-p'>[</span><span class='hljl-n'>out_id</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>op</span><span class='hljl-t'> </span><span class='hljl-oB'>===</span><span class='hljl-t'> </span><span class='hljl-n'>identity</span><span class='hljl-t'> </span><span class='hljl-oB'>?</span><span class='hljl-t'> </span><span class='hljl-n'>args</span><span class='hljl-p'>[</span><span class='hljl-ni'>1</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>:</span><span class='hljl-t'> </span><span class='hljl-nf'>op</span><span class='hljl-p'>(</span><span class='hljl-n'>args</span><span class='hljl-oB'>...</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>

    </span><span class='hljl-n'>y</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>node_vals</span><span class='hljl-p'>[</span><span class='hljl-n'>y_id</span><span class='hljl-p'>]</span><span class='hljl-t'>

    </span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>reversepass</span><span class='hljl-p'>(</span><span class='hljl-n'>ybar</span><span class='hljl-p'>)</span><span class='hljl-t'>
        </span><span class='hljl-n'>adj</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>Dict</span><span class='hljl-p'>{</span><span class='hljl-n'>Int</span><span class='hljl-p'>,</span><span class='hljl-n'>Float64</span><span class='hljl-p'>}()</span><span class='hljl-t'>
        </span><span class='hljl-n'>adj</span><span class='hljl-p'>[</span><span class='hljl-n'>y_id</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>ybar</span><span class='hljl-t'>

        </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>tape</span><span class='hljl-p'>)</span><span class='hljl-oB'>:-</span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>1</span><span class='hljl-t'>
            </span><span class='hljl-n'>op</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>in_ids</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>out_id</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>tape</span><span class='hljl-p'>[</span><span class='hljl-n'>i</span><span class='hljl-p'>]</span><span class='hljl-t'>
            </span><span class='hljl-n'>outbar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>get</span><span class='hljl-p'>(</span><span class='hljl-n'>adj</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>out_id</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.0</span><span class='hljl-p'>)</span><span class='hljl-t'>
            </span><span class='hljl-n'>args</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>ntuple</span><span class='hljl-p'>(</span><span class='hljl-n'>j</span><span class='hljl-t'> </span><span class='hljl-oB'>-&gt;</span><span class='hljl-t'> </span><span class='hljl-n'>node_vals</span><span class='hljl-p'>[</span><span class='hljl-n'>in_ids</span><span class='hljl-p'>[</span><span class='hljl-n'>j</span><span class='hljl-p'>]],</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>in_ids</span><span class='hljl-p'>))</span><span class='hljl-t'>
            </span><span class='hljl-n'>bars</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>apply_pullback</span><span class='hljl-p'>(</span><span class='hljl-n'>op</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>args</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>node_vals</span><span class='hljl-p'>[</span><span class='hljl-n'>out_id</span><span class='hljl-p'>],</span><span class='hljl-t'> </span><span class='hljl-n'>outbar</span><span class='hljl-p'>)</span><span class='hljl-t'>
            </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>j</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>id</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-nf'>enumerate</span><span class='hljl-p'>(</span><span class='hljl-n'>in_ids</span><span class='hljl-p'>)</span><span class='hljl-t'>
                </span><span class='hljl-k'>if</span><span class='hljl-t'> </span><span class='hljl-n'>id</span><span class='hljl-t'> </span><span class='hljl-oB'>∉</span><span class='hljl-t'> </span><span class='hljl-n'>is_constant</span><span class='hljl-t'>
                    </span><span class='hljl-n'>adj</span><span class='hljl-p'>[</span><span class='hljl-n'>id</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>get</span><span class='hljl-p'>(</span><span class='hljl-n'>adj</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>id</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.0</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-n'>bars</span><span class='hljl-p'>[</span><span class='hljl-n'>j</span><span class='hljl-p'>]</span><span class='hljl-t'>
                </span><span class='hljl-k'>end</span><span class='hljl-t'>
            </span><span class='hljl-k'>end</span><span class='hljl-t'>
        </span><span class='hljl-k'>end</span><span class='hljl-t'>
        </span><span class='hljl-nf'>get</span><span class='hljl-p'>(</span><span class='hljl-n'>adj</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_id</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.0</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>

    </span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>reversepass</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
f_reverse_dynamic_ad &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>pullback</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>f_reverse_dynamic_ad</span><span class='hljl-p'>(</span><span class='hljl-nfB'>2.0</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nf'>pullback</span><span class='hljl-p'>(</span><span class='hljl-nfB'>1.0</span><span class='hljl-p'>))</span>
</pre>


<pre class="output">
&#40;1.414213562373095, 0.35355339059327373&#41;
</pre>


<p>The reverse pass is completely generic — it dispatches on the recorded <code>op</code> via <code>apply_pullback</code> and accumulates adjoints by node ID. No knowledge of the specific program structure is needed. This generality is precisely why dynamic graph AD is so widely adopted: any program that can be traced produces a correct gradient.</p>
<p>The downside is visible in the implementation: we allocate a tape entry for every single scalar operation &#40;900 entries for 300 loop iterations&#41;, and every intermediate value is stored in a <code>Dict</code>. For a 300-iteration loop, this is fine; for a deep neural network with millions of operations, this overhead becomes the bottleneck.</p>
<h4>Step 4: Reverse-Mode AD — Memory-Based &#40;Checkpointing All Values&#41;</h4>
<p>Instead of recording a generic tape, we can write a reverse pass that is specialized to the structure of our program. The key insight is that the reverse pass needs the <em>values</em> from the forward pass &#40;to evaluate Jacobians&#41;, but not the tape machinery. So we simply store the intermediate values in an array:</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>f_reverse_memory</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>da</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-cs'># Forward pass: store all intermediate a values</span><span class='hljl-t'>
    </span><span class='hljl-n'>as</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>zeros</span><span class='hljl-p'>(</span><span class='hljl-ni'>301</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>as</span><span class='hljl-p'>[</span><span class='hljl-ni'>1</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-t'>
    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>300</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp1</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>as</span><span class='hljl-p'>[</span><span class='hljl-n'>i</span><span class='hljl-p'>]</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp2</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>as</span><span class='hljl-p'>[</span><span class='hljl-n'>i</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-n'>tmp1</span><span class='hljl-t'>
        </span><span class='hljl-n'>as</span><span class='hljl-p'>[</span><span class='hljl-n'>i</span><span class='hljl-oB'>+</span><span class='hljl-ni'>1</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.5</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-n'>tmp2</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>

    </span><span class='hljl-n'>y</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>as</span><span class='hljl-p'>[</span><span class='hljl-k'>end</span><span class='hljl-p'>]</span><span class='hljl-t'>

    </span><span class='hljl-cs'># Reverse pass</span><span class='hljl-t'>
    </span><span class='hljl-n'>abar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>da</span><span class='hljl-t'>
    </span><span class='hljl-n'>xbar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.0</span><span class='hljl-t'>

    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-ni'>300</span><span class='hljl-oB'>:-</span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>1</span><span class='hljl-t'>
        </span><span class='hljl-cs'># Reverse of: a[i+1] = 0.5 * tmp2</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp2bar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>abar</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.5</span><span class='hljl-t'>

        </span><span class='hljl-cs'># Reverse of: tmp2 = a[i] + tmp1</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp1bar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>tmp2bar</span><span class='hljl-t'>
        </span><span class='hljl-n'>abar_from_add</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>tmp2bar</span><span class='hljl-t'>

        </span><span class='hljl-cs'># Reverse of: tmp1 = x / a[i]</span><span class='hljl-t'>
        </span><span class='hljl-n'>xbar</span><span class='hljl-t'> </span><span class='hljl-oB'>+=</span><span class='hljl-t'> </span><span class='hljl-n'>tmp1bar</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>as</span><span class='hljl-p'>[</span><span class='hljl-n'>i</span><span class='hljl-p'>]</span><span class='hljl-t'>
        </span><span class='hljl-n'>abar_from_div</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>tmp1bar</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-oB'>-</span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>as</span><span class='hljl-p'>[</span><span class='hljl-n'>i</span><span class='hljl-p'>]</span><span class='hljl-oB'>^</span><span class='hljl-ni'>2</span><span class='hljl-p'>)</span><span class='hljl-t'>

        </span><span class='hljl-cs'># Total adjoint for a[i]</span><span class='hljl-t'>
        </span><span class='hljl-n'>abar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>abar_from_add</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-n'>abar_from_div</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>

    </span><span class='hljl-cs'># Reverse of initial: a = x</span><span class='hljl-t'>
    </span><span class='hljl-n'>xbar</span><span class='hljl-t'> </span><span class='hljl-oB'>+=</span><span class='hljl-t'> </span><span class='hljl-n'>abar</span><span class='hljl-t'>

    </span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>xbar</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
f_reverse_memory &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-nf'>f_reverse_memory</span><span class='hljl-p'>(</span><span class='hljl-nfB'>2.0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nfB'>1.0</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
&#40;1.414213562373095, 0.35355339059327373&#41;
</pre>


<p>The reverse pass is now just a loop — no dictionaries, no dispatch, no heap allocations per operation. Each line of the reverse loop is the adjoint of the corresponding line in the forward loop, read in reverse order:</p>
<table><tr><th>Forward</th><th>Reverse</th></tr><tr><td><code>a&#91;i&#43;1&#93; &#61; 0.5 * tmp2</code></td><td><code>tmp2bar &#61; abar * 0.5</code></td></tr><tr><td><code>tmp2 &#61; a&#91;i&#93; &#43; tmp1</code></td><td><code>abar &#43;&#61; tmp2bar; tmp1bar &#61; tmp2bar</code></td></tr><tr><td><code>tmp1 &#61; x / a&#91;i&#93;</code></td><td><code>xbar &#43;&#61; tmp1bar/a&#91;i&#93;; abar &#43;&#61; -tmp1bar*x/a&#91;i&#93;^2</code></td></tr></table>
<p>Note how <code>xbar</code> accumulates a contribution from every iteration, since <code>x</code> is used in <code>tmp1 &#61; x/a</code> at every step. This is the &quot;fan-out&quot; rule: when a variable is used multiple times, its adjoint is the <em>sum</em> of all contributions.</p>
<p>The cost is <span class="math">$O(N)$</span> memory to store the <code>as</code> array, where <span class="math">$N$</span> is the number of iterations.</p>
<h4>Step 5: Reverse-Mode AD — Memoryless &#40;Enzyme/NiLang Style&#41;</h4>
<p>Source-to-source AD systems like Enzyme and NiLang can avoid storing intermediates entirely by <em>inverting</em> the forward computation. If we can reconstruct <span class="math">$a_{\text{old}}$</span> from <span class="math">$a_{\text{new}}$</span>, we don&#39;t need the <code>as</code> array at all.</p>
<p>The forward step is <span class="math">$a_{\text{new}} = \frac{1}{2}(a_{\text{old}} + x/a_{\text{old}})$</span>. Inverting: <span class="math">$\text{tmp2} = 2 a_{\text{new}}$</span>, and <span class="math">$a_{\text{old}}$</span> satisfies <span class="math">$a_{\text{old}}^2 - \text{tmp2} \cdot a_{\text{old}} + x = 0$</span>, giving <span class="math">$a_{\text{old}} = \frac{\text{tmp2} + \sqrt{\text{tmp2}^2 - 4x}}{2}$</span> &#40;taking the larger root, since <span class="math">$a > \sqrt{x}$</span> throughout the iteration&#41;:</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>f_reverse_memoryless</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>da</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-t'>
    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>300</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp1</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>a</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp2</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-n'>tmp1</span><span class='hljl-t'>
        </span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.5</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-n'>tmp2</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>

    </span><span class='hljl-n'>aout</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>a</span><span class='hljl-t'>

    </span><span class='hljl-cs'># Reverse pass: reconstruct intermediates by inverting each step</span><span class='hljl-t'>
    </span><span class='hljl-n'>abar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>da</span><span class='hljl-t'>
    </span><span class='hljl-n'>xbar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.0</span><span class='hljl-t'>

    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-ni'>300</span><span class='hljl-oB'>:-</span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>1</span><span class='hljl-t'>
        </span><span class='hljl-cs'># Invert: a_new = 0.5 * tmp2 =&gt; tmp2 = 2 * a</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp2</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-n'>a</span><span class='hljl-t'>
        </span><span class='hljl-cs'># Invert: a_old^2 - tmp2*a_old + x = 0 =&gt; take larger root</span><span class='hljl-t'>
        </span><span class='hljl-n'>a_old</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>tmp2</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-nf'>sqrt</span><span class='hljl-p'>(</span><span class='hljl-nf'>abs</span><span class='hljl-p'>(</span><span class='hljl-n'>tmp2</span><span class='hljl-oB'>^</span><span class='hljl-ni'>2</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-ni'>4</span><span class='hljl-n'>x</span><span class='hljl-p'>)))</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp1</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>a_old</span><span class='hljl-t'>

        </span><span class='hljl-n'>tmp2bar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>abar</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.5</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp1bar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>tmp2bar</span><span class='hljl-t'>
        </span><span class='hljl-n'>abar_from_add</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>tmp2bar</span><span class='hljl-t'>

        </span><span class='hljl-n'>xbar</span><span class='hljl-t'> </span><span class='hljl-oB'>+=</span><span class='hljl-t'> </span><span class='hljl-n'>tmp1bar</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>a_old</span><span class='hljl-t'>
        </span><span class='hljl-n'>abar_from_div</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>tmp1bar</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-oB'>-</span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>a_old</span><span class='hljl-oB'>^</span><span class='hljl-ni'>2</span><span class='hljl-p'>)</span><span class='hljl-t'>

        </span><span class='hljl-n'>abar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>abar_from_add</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-n'>abar_from_div</span><span class='hljl-t'>
        </span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>a_old</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>

    </span><span class='hljl-n'>xbar</span><span class='hljl-t'> </span><span class='hljl-oB'>+=</span><span class='hljl-t'> </span><span class='hljl-n'>abar</span><span class='hljl-t'>

    </span><span class='hljl-n'>aout</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>xbar</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
f_reverse_memoryless &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-nf'>f_reverse_memoryless</span><span class='hljl-p'>(</span><span class='hljl-nfB'>2.0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nfB'>1.0</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
&#40;1.414213562373095, 0.35355339059327234&#41;
</pre>


<p>This uses <span class="math">$O(1)$</span> memory regardless of the number of iterations. The tradeoff is that the inversion &#40;here, a quadratic formula&#41; adds computation and can introduce small floating-point errors. For the Babylonian method specifically, the inversion is exact in exact arithmetic but introduces ~<span class="math">$10^{-15}$</span> error in floating point — acceptable for most applications.</p>
<h4>Step 6: Dynamic Control Flow</h4>
<p>What happens when the iteration count isn&#39;t fixed? Replace the <code>for</code> loop with a convergence-based <code>while</code> loop:</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>f_while</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-t'>
    </span><span class='hljl-k'>while</span><span class='hljl-t'> </span><span class='hljl-nf'>abs</span><span class='hljl-p'>(</span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-oB'>/</span><span class='hljl-n'>a</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>&gt;</span><span class='hljl-t'> </span><span class='hljl-nfB'>1e-14</span><span class='hljl-t'>
        </span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.5</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-oB'>/</span><span class='hljl-n'>a</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
    </span><span class='hljl-n'>a</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
f_while &#40;generic function with 1 method&#41;
</pre>


<p>This changes nothing about the derivative rules — only about what the AD system must handle. For memory-based reverse AD, we store values as before but use a growable array:</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>f_while_reverse_memory</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>da</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>as</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>[</span><span class='hljl-n'>x</span><span class='hljl-p'>]</span><span class='hljl-t'>
    </span><span class='hljl-k'>while</span><span class='hljl-t'> </span><span class='hljl-nf'>abs</span><span class='hljl-p'>(</span><span class='hljl-n'>as</span><span class='hljl-p'>[</span><span class='hljl-k'>end</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>as</span><span class='hljl-p'>[</span><span class='hljl-k'>end</span><span class='hljl-p'>])</span><span class='hljl-t'> </span><span class='hljl-oB'>&gt;</span><span class='hljl-t'> </span><span class='hljl-nfB'>1e-14</span><span class='hljl-t'>
        </span><span class='hljl-n'>a_prev</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>as</span><span class='hljl-p'>[</span><span class='hljl-k'>end</span><span class='hljl-p'>]</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp1</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>a_prev</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp2</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>a_prev</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-n'>tmp1</span><span class='hljl-t'>
        </span><span class='hljl-nf'>push!</span><span class='hljl-p'>(</span><span class='hljl-n'>as</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.5</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-n'>tmp2</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>

    </span><span class='hljl-n'>y</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>as</span><span class='hljl-p'>[</span><span class='hljl-k'>end</span><span class='hljl-p'>]</span><span class='hljl-t'>
    </span><span class='hljl-n'>abar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>da</span><span class='hljl-t'>
    </span><span class='hljl-n'>xbar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.0</span><span class='hljl-t'>

    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>as</span><span class='hljl-p'>))</span><span class='hljl-oB'>:-</span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>2</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp2bar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>abar</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.5</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp1bar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>tmp2bar</span><span class='hljl-t'>
        </span><span class='hljl-n'>abar_from_add</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>tmp2bar</span><span class='hljl-t'>

        </span><span class='hljl-n'>xbar</span><span class='hljl-t'> </span><span class='hljl-oB'>+=</span><span class='hljl-t'> </span><span class='hljl-n'>tmp1bar</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>as</span><span class='hljl-p'>[</span><span class='hljl-n'>i</span><span class='hljl-oB'>-</span><span class='hljl-ni'>1</span><span class='hljl-p'>]</span><span class='hljl-t'>
        </span><span class='hljl-n'>abar_from_div</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>tmp1bar</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-oB'>-</span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>as</span><span class='hljl-p'>[</span><span class='hljl-n'>i</span><span class='hljl-oB'>-</span><span class='hljl-ni'>1</span><span class='hljl-p'>]</span><span class='hljl-oB'>^</span><span class='hljl-ni'>2</span><span class='hljl-p'>)</span><span class='hljl-t'>

        </span><span class='hljl-n'>abar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>abar_from_add</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-n'>abar_from_div</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>

    </span><span class='hljl-n'>xbar</span><span class='hljl-t'> </span><span class='hljl-oB'>+=</span><span class='hljl-t'> </span><span class='hljl-n'>abar</span><span class='hljl-t'>
    </span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>xbar</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
f_while_reverse_memory &#40;generic function with 1 method&#41;
</pre>


<p>For the memoryless version, we just need to count iterations:</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>f_while_reverse_memoryless</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>da</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-t'>
    </span><span class='hljl-n'>iters</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>0</span><span class='hljl-t'>
    </span><span class='hljl-k'>while</span><span class='hljl-t'> </span><span class='hljl-nf'>abs</span><span class='hljl-p'>(</span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-oB'>/</span><span class='hljl-n'>a</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>&gt;</span><span class='hljl-t'> </span><span class='hljl-nfB'>1e-14</span><span class='hljl-t'>
        </span><span class='hljl-n'>iters</span><span class='hljl-t'> </span><span class='hljl-oB'>+=</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp1</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>a</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp2</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-n'>tmp1</span><span class='hljl-t'>
        </span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.5</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-n'>tmp2</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>

    </span><span class='hljl-n'>aout</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>a</span><span class='hljl-t'>
    </span><span class='hljl-n'>abar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>da</span><span class='hljl-t'>
    </span><span class='hljl-n'>xbar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.0</span><span class='hljl-t'>

    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-n'>iters</span><span class='hljl-oB'>:-</span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>1</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp2</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-n'>a</span><span class='hljl-t'>
        </span><span class='hljl-n'>a_old</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>tmp2</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-nf'>sqrt</span><span class='hljl-p'>(</span><span class='hljl-nf'>abs</span><span class='hljl-p'>(</span><span class='hljl-n'>tmp2</span><span class='hljl-oB'>^</span><span class='hljl-ni'>2</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-ni'>4</span><span class='hljl-n'>x</span><span class='hljl-p'>)))</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp1</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>a_old</span><span class='hljl-t'>

        </span><span class='hljl-n'>tmp2bar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>abar</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.5</span><span class='hljl-t'>
        </span><span class='hljl-n'>tmp1bar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>tmp2bar</span><span class='hljl-t'>
        </span><span class='hljl-n'>abar_from_add</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>tmp2bar</span><span class='hljl-t'>

        </span><span class='hljl-n'>xbar</span><span class='hljl-t'> </span><span class='hljl-oB'>+=</span><span class='hljl-t'> </span><span class='hljl-n'>tmp1bar</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>a_old</span><span class='hljl-t'>
        </span><span class='hljl-n'>abar_from_div</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>tmp1bar</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-oB'>-</span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-n'>a_old</span><span class='hljl-oB'>^</span><span class='hljl-ni'>2</span><span class='hljl-p'>)</span><span class='hljl-t'>

        </span><span class='hljl-n'>abar</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>abar_from_add</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-n'>abar_from_div</span><span class='hljl-t'>
        </span><span class='hljl-n'>a</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>a_old</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>

    </span><span class='hljl-n'>xbar</span><span class='hljl-t'> </span><span class='hljl-oB'>+=</span><span class='hljl-t'> </span><span class='hljl-n'>abar</span><span class='hljl-t'>
    </span><span class='hljl-n'>aout</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>xbar</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
f_while_reverse_memoryless &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-nf'>f_while_reverse_memory</span><span class='hljl-p'>(</span><span class='hljl-nfB'>2.0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nfB'>1.0</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
&#40;1.414213562373095, 0.35355339059327373&#41;
</pre>



<pre class='hljl'>
<span class='hljl-nf'>f_while_reverse_memoryless</span><span class='hljl-p'>(</span><span class='hljl-nfB'>2.0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nfB'>1.0</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
&#40;1.414213562373095, 0.35355339059327473&#41;
</pre>


<p>Dynamic control flow is where the tradeoffs between AD strategies become sharp:</p>
<ul>
<li><p><strong>Tracing-based AD</strong> &#40;PyTorch, JAX&#41; unrolls the while loop into a flat trace. The trace length varies per input — you cannot JIT compile it once.</p>
</li>
<li><p><strong>Memory-based AD</strong> stores values as the loop runs. The memory cost is proportional to the &#40;unknown in advance&#41; iteration count.</p>
</li>
<li><p><strong>Memoryless/source-to-source AD</strong> &#40;Enzyme&#41; only needs to record the iteration count &#40;a single integer&#41;, then inverts each step. This is why Enzyme can differentiate through complex loops and solvers with minimal overhead.</p>
</li>
</ul>
<h4>Summary: Comparing the Strategies</h4>
<p>Let&#39;s verify all implementations against the exact derivative:</p>


<pre class='hljl'>
<span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>2.0</span><span class='hljl-t'>
</span><span class='hljl-n'>exact</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-t'> </span><span class='hljl-oB'>/</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-ni'>2</span><span class='hljl-oB'>*</span><span class='hljl-nf'>sqrt</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>))</span><span class='hljl-t'>

</span><span class='hljl-n'>_</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>d_fwd</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>f_forward</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nfB'>1.0</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-n'>_</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>d_mem</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>f_reverse_memory</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nfB'>1.0</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-n'>_</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>d_mless</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>f_reverse_memoryless</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nfB'>1.0</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-n'>_</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>pb</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>f_reverse_dynamic_ad</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-n'>d_dyn</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>pb</span><span class='hljl-p'>(</span><span class='hljl-nfB'>1.0</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-n'>_</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>d_wmem</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>f_while_reverse_memory</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nfB'>1.0</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-n'>_</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>d_wmless</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>f_while_reverse_memoryless</span><span class='hljl-p'>(</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nfB'>1.0</span><span class='hljl-p'>)</span><span class='hljl-t'>

</span><span class='hljl-nf'>println</span><span class='hljl-p'>(</span><span class='hljl-s'>&quot;Exact derivative:        </span><span class='hljl-si'>$exact</span><span class='hljl-s'>&quot;</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nf'>println</span><span class='hljl-p'>(</span><span class='hljl-s'>&quot;Forward-mode:            </span><span class='hljl-si'>$d_fwd</span><span class='hljl-s'>    (err = </span><span class='hljl-si'>$</span><span class='hljl-p'>(</span><span class='hljl-nf'>abs</span><span class='hljl-p'>(</span><span class='hljl-n'>d_fwd</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-n'>exact</span><span class='hljl-p'>))</span><span class='hljl-s'>)&quot;</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nf'>println</span><span class='hljl-p'>(</span><span class='hljl-s'>&quot;Reverse dynamic (tape):  </span><span class='hljl-si'>$d_dyn</span><span class='hljl-s'>    (err = </span><span class='hljl-si'>$</span><span class='hljl-p'>(</span><span class='hljl-nf'>abs</span><span class='hljl-p'>(</span><span class='hljl-n'>d_dyn</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-n'>exact</span><span class='hljl-p'>))</span><span class='hljl-s'>)&quot;</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nf'>println</span><span class='hljl-p'>(</span><span class='hljl-s'>&quot;Reverse memory:          </span><span class='hljl-si'>$d_mem</span><span class='hljl-s'>    (err = </span><span class='hljl-si'>$</span><span class='hljl-p'>(</span><span class='hljl-nf'>abs</span><span class='hljl-p'>(</span><span class='hljl-n'>d_mem</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-n'>exact</span><span class='hljl-p'>))</span><span class='hljl-s'>)&quot;</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nf'>println</span><span class='hljl-p'>(</span><span class='hljl-s'>&quot;Reverse memoryless:      </span><span class='hljl-si'>$d_mless</span><span class='hljl-s'>  (err = </span><span class='hljl-si'>$</span><span class='hljl-p'>(</span><span class='hljl-nf'>abs</span><span class='hljl-p'>(</span><span class='hljl-n'>d_mless</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-n'>exact</span><span class='hljl-p'>))</span><span class='hljl-s'>)&quot;</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nf'>println</span><span class='hljl-p'>(</span><span class='hljl-s'>&quot;While + memory:          </span><span class='hljl-si'>$d_wmem</span><span class='hljl-s'>   (err = </span><span class='hljl-si'>$</span><span class='hljl-p'>(</span><span class='hljl-nf'>abs</span><span class='hljl-p'>(</span><span class='hljl-n'>d_wmem</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-n'>exact</span><span class='hljl-p'>))</span><span class='hljl-s'>)&quot;</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nf'>println</span><span class='hljl-p'>(</span><span class='hljl-s'>&quot;While + memoryless:      </span><span class='hljl-si'>$d_wmless</span><span class='hljl-s'> (err = </span><span class='hljl-si'>$</span><span class='hljl-p'>(</span><span class='hljl-nf'>abs</span><span class='hljl-p'>(</span><span class='hljl-n'>d_wmless</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-n'>exact</span><span class='hljl-p'>))</span><span class='hljl-s'>)&quot;</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
Exact derivative:        0.35355339059327373
Forward-mode:            0.35355339059327373    &#40;err &#61; 0.0&#41;
Reverse dynamic &#40;tape&#41;:  0.35355339059327373    &#40;err &#61; 0.0&#41;
Reverse memory:          0.35355339059327373    &#40;err &#61; 0.0&#41;
Reverse memoryless:      0.35355339059327234  &#40;err &#61; 1.3877787807814457e-15
&#41;
While &#43; memory:          0.35355339059327373   &#40;err &#61; 0.0&#41;
While &#43; memoryless:      0.35355339059327473 &#40;err &#61; 9.992007221626409e-16&#41;
</pre>


<table><tr><th>Strategy</th><th>Memory</th><th>Tape overhead</th><th>Handles dynamic control flow</th><th>Examples</th></tr><tr><td>Forward-mode</td><td><span class="math">$O(1)$</span></td><td>None</td><td>Yes &#40;naturally&#41;</td><td>ForwardDiff.jl, dual numbers</td></tr><tr><td>Reverse dynamic</td><td><span class="math">$O(N)$</span></td><td>Dict &#43; alloc</td><td>Yes &#40;re-traces each call&#41;</td><td>PyTorch, JAX, Tracker.jl, ReverseDiff.jl</td></tr><tr><td>Reverse memory</td><td><span class="math">$O(N)$</span></td><td>None</td><td>Yes &#40;growable arrays&#41;</td><td>Checkpointing schemes</td></tr><tr><td>Reverse memoryless</td><td><span class="math">$O(1)$</span></td><td>None</td><td>Yes &#40;count only&#41;</td><td>Enzyme, NiLang</td></tr><tr><td>Source-to-source</td><td>Varies</td><td>None</td><td>Yes &#40;with analysis&#41;</td><td>Zygote.jl, Enzyme, Tapenade</td></tr></table>
<h2>Derivation of Reverse Mode Rules: Adjoints and Implicit Function Theorem</h2>
<p>Now this shows how reverse-mode AD generally works, and we can see from the general implementation that the key is to implement <code>apply_pullback</code> rules on specific primitive operations. This is just like the primitive rules for forward-mode AD, except now computing the vector-Jacobian product. </p>
<p>While this is easy to do for simple mathematical operations, you may be asking, if I have a complicated function, how do I derive the <code>apply_pullback</code> rules? It terns out that this intersects with an area of computational science known as adjoint methods. Adjoint methods are <code>apply_pullback</code> reverse-mode AD rules in disguise&#33;</p>
<p>In order to require the least amount of work from our AD system, we need to be able to derive the adjoint rules at the highest level possible. Here are a few well-known cases to start understanding. These next examples are from <a href="https://math.mit.edu/~stevenj/18.336/adjoint.pdf">Steven Johnson&#39;s resource</a>.</p>
<p>Let&#39;s go through the full derivation of a few.</p>
<h3>Adjoint of Linear Solve</h3>
<p>Let&#39;s say we have the function <span class="math">$A(p)x=b(p)$</span>, i.e. this is the function that is given by the linear solving process, and we want to calculate the gradients of a cost function <span class="math">$g(x,p)$</span>. To evaluate the gradient directly, we&#39;d calculate:</p>
<p class="math">\[
\frac{dg}{dp} = g_p + g_x x_p
\]</p>
<p>where <span class="math">$x_p$</span> is the derivative of each value of <span class="math">$x$</span> with respect to each parameter <span class="math">$p$</span>, and thus it&#39;s an <span class="math">$M \times P$</span> matrix &#40;a Jacobian&#41;. Since <span class="math">$g$</span> is a small cost function, <span class="math">$g_p$</span> and <span class="math">$g_x$</span> are easy to compute, but <span class="math">$x_p$</span> is given by:</p>
<p class="math">\[
x_{p_i} = A^{-1}(b_{p_i}-A_{p_i}x)
\]</p>
<p>and so this is <span class="math">$P$</span> <span class="math">$M \times M$</span> linear solves, which is expensive&#33; However, if we multiply by</p>
<p class="math">\[
\lambda^{T} = g_x A^{-1}
\]</p>
<p>then we obtain</p>
<p class="math">\[
\frac{dg}{dp}\vert_{f=0} = g_p - \lambda^T f_p = g_p - \lambda^T (A_p x - b_p)
\]</p>
<p>which is an alternative formulation of the derivative at the solution value. However, in this case, there is no computational benefit to this reformulation.</p>
<h3>Adjoint of Nonlinear Solve</h3>
<p>Now let&#39;s look at some <span class="math">$f(x,p)=0$</span> nonlinear solving. Differentiating by <span class="math">$p$</span> gives us:</p>
<p class="math">\[
f_x x_p + f_p = 0
\]</p>
<p>and thus <span class="math">$x_p = -f_x^{-1}f_p$</span>. Therefore, using our cost function we write:</p>
<p class="math">\[
\frac{dg}{dp} = g_p + g_x x_p = g_p - g_x \left(f_x^{-1} f_p \right)
\]</p>
<p>or</p>
<p class="math">\[
\frac{dg}{dp} = g_p - \left(g_x f_x^{-1} \right) f_p
\]</p>
<p>Since <span class="math">$g_x$</span> is <span class="math">$1 \times M$</span>, <span class="math">$f_x^{-1}$</span> is <span class="math">$M \times M$</span>, and <span class="math">$f_p$</span> is <span class="math">$M \times P$</span>, this grouping changes the problem and gets rid of the size <span class="math">$MP$</span> term.</p>
<p>As is normal with backpasses, we solve for <span class="math">$x$</span> through the forward pass however we like, and then for the backpass solve for</p>
<p class="math">\[
f_x^T \lambda = g_x^T
\]</p>
<p>to obtain</p>
<p class="math">\[
\frac{dg}{dp}\vert_{f=0} = g_p - \lambda^T f_p
\]</p>
<p>which does the calculation without ever building the size <span class="math">$M \times MP$</span> term.</p>
<h3>Adjoint of Ordinary Differential Equations</h3>
<p>We wish to solve for some cost function <span class="math">$G(u,p)$</span> evaluated throughout the differential equation, i.e.:</p>
<p class="math">\[
G(u,p) = G(u(p)) = \int_{t_0}^T g(u(t,p))dt
\]</p>
<p>To derive this adjoint, introduce the Lagrange multiplier <span class="math">$\lambda$</span> to form:</p>
<p class="math">\[
I(p) = G(p) - \int_{t_0}^T \lambda^\ast (u^\prime - f(u,p,t))dt
\]</p>
<p>Since <span class="math">$u^\prime = f(u,p,t)$</span>, this is the mathematician&#39;s trick of adding zero, so then we have that</p>
<p class="math">\[
\frac{dG}{dp} = \frac{dI}{dp} = \int_{t_0}^T (g_p + g_u s)dt - \int_{t_0}^T \lambda^\ast (s^\prime - f_u s - f_p)dt
\]</p>
<p>for <span class="math">$s$</span> being the sensitivity, <span class="math">$s = \frac{du}{dp}$</span>. After applying integration by parts to <span class="math">$\lambda^\ast s^\prime$</span>, we get that:</p>
<p class="math">\[
\int_{t_{0}}^{T}\lambda^{\ast}\left(s^{\prime}-f_{u}s-f_{p}\right)dt	=\int_{t_{0}}^{T}\lambda^{\ast}s^{\prime}dt-\int_{t_{0}}^{T}\lambda^{\ast}\left(f_{u}s+f_{p}\right)dt
\]</p>
<p class="math">\[
=|\lambda^{\ast}(t)s(t)|_{t_{0}}^{T}-\int_{t_{0}}^{T}\lambda^{\ast\prime}sdt-\int_{t_{0}}^{T}\lambda^{\ast}\left(f_{u}s+f_{p}\right)dt
\]</p>
<p>To see where we ended up, let&#39;s re-arrange the full expression now:</p>
<p class="math">\[
\frac{dG}{dp}	=\int_{t_{0}}^{T}(g_{p}+g_{u}s)dt+|\lambda^{\ast}(t)s(t)|_{t_{0}}^{T}-\int_{t_{0}}^{T}\lambda^{\ast\prime}sdt-\int_{t_{0}}^{T}\lambda^{\ast}\left(f_{u}s+f_{p}\right)dt
\]</p>
<p class="math">\[
=\int_{t_{0}}^{T}(g_{p}+\lambda^{\ast}f_{p})dt+|\lambda^{\ast}(t)s(t)|_{t_{0}}^{T}-\int_{t_{0}}^{T}\left(\lambda^{\ast\prime}+\lambda^\ast f_{u}-g_{u}\right)sdt
\]</p>
<p>That was just a re-arrangement. Now, let&#39;s require that</p>
<p class="math">\[
\lambda^\prime = -\frac{df}{du}^\ast \lambda + \left(\frac{dg}{du} \right)^\ast
\]</p>
<p class="math">\[
\lambda(T) = 0
\]</p>
<p>This means that one of the boundary terms of the integration by parts is zero, and also one of those integrals is perfectly zero. Thus, if <span class="math">$\lambda$</span> satisfies that equation, then we get:</p>
<p class="math">\[
\frac{dG}{dp} = \lambda^\ast(t_0)\frac{du(t_0)}{dp} + \int_{t_0}^T \left(g_p + \lambda^\ast f_p \right)dt
\]</p>
<p>which gives us our adjoint derivative relation.</p>
<p>If <span class="math">$G$</span> is discrete, then it can be represented via the Dirac delta:</p>
<p class="math">\[
G(u,p) = \int_{t_0}^T \sum_{i=1}^N \Vert d_i - u(t_i,p)\Vert^2 \delta(t_i - t)dt
\]</p>
<p>in which case</p>
<p class="math">\[
g_u(t_i) = 2(d_i - u(t_i,p))
\]</p>
<p>at the data points <span class="math">$(t_i,d_i)$</span>. Therefore, the derivatives of a cost function with respect to the parameters are obtained by solving for <span class="math">$\lambda^\ast$</span> using an ODE for <span class="math">$\lambda^T$</span> in reverse time, and then using that to calculate <span class="math">$\frac{dG}{dp}$</span>. Note that <span class="math">$\frac{dG}{dp}$</span> can be calculated simultaneously by appending a single value to the reverse ODE, since we can simply define the new ODE term as <span class="math">$g_p + \lambda^\ast f_p$</span>, which would then calculate the integral on the fly &#40;ODE integration is just... integration&#33;&#41;.</p>
<h3>Complexities of Implementing ODE Adjoints</h3>
<p>The image below explains the dilemma:</p>
<p><img src="https://user-images.githubusercontent.com/1814174/66882662-4d741a00-ef99-11e9-9233-4d6804fec2ec.PNG" alt="" /></p>
<p>Essentially, the whole problem is that we need to solve the ODE</p>
<p class="math">\[
\lambda^\prime = -\frac{df}{du}^\ast \lambda - \left(\frac{dg}{du} \right)^\ast
\]</p>
<p class="math">\[
\lambda(T) = 0
\]</p>
<p>in reverse, but <span class="math">$\frac{df}{du}$</span> is defined by <span class="math">$u(t)$</span> which is a value only computed in the forward pass &#40;the forward pass is embedded within the backpass&#33;&#41;. Thus we need to be able to retrieve the value of <span class="math">$u(t)$</span> to get the Jacobian on-demand. There are three ways in which this can be done:</p>
<ol>
<li><p>If you solve the reverse ODE <span class="math">$u^\prime = f(u,p,t)$</span> backwards in time, mathematically it&#39;ll give equivalent values. Computation-wise, this means that you can append <span class="math">$u(t)$</span> to <span class="math">$\lambda(t)$</span> &#40;to <span class="math">$\frac{dG}{dp}$</span>&#41; to calculate all terms at the same time with a single reverse pass ODE. However, numerically this is unstable and thus not always recommended &#40;ODEs are reversible, but ODE solver methods are not necessarily going to generate the same exact values or trajectories in reverse&#33;&#41;</p>
</li>
<li><p>If you solve the forward ODE and receive a solution <span class="math">$u(t)$</span>, you can interpolate it to retrieve the values at any time at which the reverse pass needs the <span class="math">$\frac{df}{du}$</span> Jacobian. This is fast but memory-intensive.</p>
</li>
<li><p>Every time you need a value <span class="math">$u(t)$</span> during the backpass, you re-solve the forward ODE to <span class="math">$u(t)$</span>. This is expensive&#33; Thus one can instead use <em>checkpoints</em>, i.e. save at a smaller number of time points during the forward pass, and use those as starting points for the <span class="math">$u(t)$</span> calculation.</p>
</li>
</ol>
<p>Alternative strategies can be investigated, such as an interpolation that stores values in a compressed form.</p>
<h3>The vjp and Neural Ordinary Differential Equations</h3>
<p>It is here that we can note that, if <span class="math">$f$</span> is a function defined by a neural network, we arrive at the <em>neural ordinary differential equation</em>. This adjoint method is thus the backpropagation method for the neural ODE. However, the backpass</p>
<p class="math">\[
\lambda^\prime = -\frac{df}{du}^\ast \lambda - \left(\frac{dg}{du} \right)^\ast
\]</p>
<p class="math">\[
\lambda(T) = 0
\]</p>
<p>can be improved by noticing <span class="math">$\lambda^\ast \frac{df}{du}$</span> is a vjp, and thus it can be calculated using <span class="math">$\mathcal{B}_f^{u(t)}(\lambda^\ast)$</span>, i.e. reverse-mode AD on the function <span class="math">$f$</span>. If <span class="math">$f$</span> is a neural network, this means that the reverse ODE is defined through successive backpropagation passes of that neural network. The result is a derivative of the cost function  with respect to the parameters defining <span class="math">$f$</span> &#40;either a model or a neural network&#41;, which can then be used to fit the data &#40;&quot;train&quot;&#41;.</p>
<h2>Alternative &quot;Training&quot; Strategies</h2>
<p>Those are the &quot;brute force&quot; training methods which simply use <span class="math">$u(t,p)$</span> evaluations to calculate the cost. However, it is worth noting that there are a few better strategies that one can employ in the case of dynamical models.</p>
<h3>Multiple Shooting Techniques</h3>
<p>Instead of shooting just from the beginning, one can instead shoot from multiple points in time:</p>
<p><img src="https://user-images.githubusercontent.com/1814174/66883548-561a1f80-ef9c-11e9-9ce1-0b6b55c950f9.PNG" alt="" /></p>
<p>Of course, one won&#39;t know what the &quot;initial condition in the future&quot; is, but one can instead make that a parameter. By doing so, each interval can be solved independently, and one can then add to the cost function that the end of one interval must match up with the beginning of the other. This can make the integration more robust, since shooting with incorrect parameters over long time spans can give massive gradients which makes it hard to hone in on the correct values.</p>
<h3>Collocation Methods</h3>
<p>If the data is dense enough, one can fit a curve through the points, such as a spline:</p>
<p><img src="https://user-images.githubusercontent.com/1814174/66883762-fc662500-ef9c-11e9-91c7-c445e32d120f.PNG" alt="" /></p>
<p>If that&#39;s the case, one can use the fit spline in order to estimate the derivative at each point. Since the ODE is defined as <span class="math">$u^\prime = f(u,p,t)$</span>, one can then use the cost function</p>
<p class="math">\[
C(p) = \sum_{i=1}^N \Vert\tilde{u}^{\prime}(t_i) - f(u(t_i),p,t)\Vert
\]</p>
<p>where <span class="math">$\tilde{u}^{\prime}(t_i)$</span> is the estimated derivative at the time point <span class="math">$t_i$</span>. Then one can fit the parameters to ensure this holds. This method can be extremely fast since the ODE doesn&#39;t ever have to be solved&#33; However, note that this is not able to compensate for error accumulation, and thus early errors are not accounted for in the later parts of the data. This means that the integration won&#39;t necessarily match the data even if this fit is &quot;good&quot; if the data points are too far apart, a property that is not true with fitting. Thus, this is usually done as part of a <em>two-stage method</em>, where the starting stage uses collocation to get initial parameters which is then completed with a shooting method.</p>


<div class="footer">
  <p>
    Published from <a href="adjoints.jmd">adjoints.jmd</a>
    using <a href="http://github.com/JunoLab/Weave.jl">Weave.jl</a> v0.10.12 on 2026-03-01.
  </p>
</div>
