<h1 class=title >Homework 1, Parallelized Dynamics</h1> <h5>Chris Rackauckas</h5> <h5>September 15th, 2020</h5> <p>Due October 1st, 2020 at midnight EST.</p> <p>Homework 1 is a chance to get some experience implementing discrete dynamical systems techniques in a way that is parallelized, and a time to understand the fundamental behavior of the bottleneck algorithms in scientific computing.</p> <h2>Problem 1: A Ton of New Facts on Newton</h2> <p>In lecture 4 we looked at the properties of discrete dynamical systems to see that running many systems for infinitely many steps would go to a steady state. This process is used as a numerical method known as <strong>fixed point iteration</strong> to solve for the steady state of systems <span class=math >$x_{n+1} = f(x_{n})$</span>. Under a transformation &#40;which we will do in this homework&#41;, it can be used to solve rootfinding problems <span class=math >$f(x) = 0$</span> to solve for <span class=math >$x$</span>.</p> <p>In this problem we will look into Newton&#39;s method. Newton&#39;s method is the dynamical system defined by the update process:</p> <p class=math >\[ x_{n+1} = x_n - \left(\frac{dg}{dx}(x_n)\right)^{-1} g(x_n) \]</p> <p>For these problems, assume that <span class=math >$\frac{dg}{dx}$</span> is non-singular. We will prove a few properties to show why, in practice, Newton methods are preferred for quickly calculating the steady state.</p> <h3>Part 1</h3> <p>Show that if <span class=math >$x^\ast$</span> is a steady state of the equation, then <span class=math >$g(x^\ast) = 0$</span>.</p> <h3>Part 2</h3> <p>Take a look at the Quasi-Newton approximation:</p> <p class=math >\[ x_{n+1} = x_n - \left(\frac{dg}{dx}(x_0)\right)^{-1} g(x_n) \]</p> <p>for some fixed <span class=math >$x_0$</span>. Derive the stability of the Quasi-Newton approximation in the form of a matrix whose eigenvalues need to be constrained. Use this to argue that if <span class=math >$x_0$</span> is sufficiently close to <span class=math >$x^\ast$</span> then the steady state is a stable &#40;attracting&#41; steady state.</p> <h3>Part 3</h3> <p>Relaxed Quasi-Newton is the method:</p> <p class=math >\[ x_{n+1} = x_n - \alpha \left(\frac{dg}{dx}(x_0)\right)^{-1} g(x_n) \]</p> <p>Argue that for some sufficiently small <span class=math >$\alpha$</span> that the Quasi-Newton iterations will be stable if the eigenvalues of <span class=math >$(\left(\frac{dg}{dx}(x_0)\right)^{-1} g(x_n))^\prime$</span> are all positive for every <span class=math >$x$</span>.</p> <p>&#40;Technically, these assumptions can be greatly relaxed, but weird cases arise. When <span class=math >$x \in \mathbb{C}$</span>, this holds except on some set of Lebesgue measure zero. Feel free to explore this.&#41;</p> <h3>Part 4</h3> <p>Fixed point iteration is the dynamical system</p> <p class=math >\[ x_{n+1} = g(x_n) \]</p> <p>which converges to <span class=math >$g(x)=x$</span>.</p> <ol> <li><p>What is a small change to the dynamical system that could be done such that <span class=math >$g(x)=0$</span> is the steady state?</p> <li><p>How can you change the <span class=math >$\left(\frac{dg}{dx}(x_0)\right)^{-1}$</span> term from the Quasi-Newton iteration to get a method equivalent to fixed point iteration? What does this imply about the difference in stability between Quasi-Newton and fixed point iteration if <span class=math >$\frac{dg}{dx}$</span> has large eigenvalues?</p> </ol> <h2>Problem 2: The Root of all Problems</h2> <p>In this problem we will practice writing fast and type-generic Julia code by producing an algorithm that will compute the quantile of any probability distribution.</p> <h3>Part 1</h3> <p>Many problems can be interpreted as a rootfinding problem. For example, let&#39;s take a look at a problem in statistics. Let <span class=math >$X$</span> be a random variable with a cumulative distribution function &#40;CDF&#41; of <span class=math >$cdf(x)$</span>. Recall that the CDF is a monotonically increasing function in <span class=math >$[0,1]$</span> which is the total probability of <span class=math >$X < x$</span>. The <span class=math >$y$</span>th quantile of <span class=math >$X$</span> is the value <span class=math >$x$</span> at with <span class=math >$X$</span> has a y&#37; chance of being less than <span class=math >$x$</span>. Interpret the problem of computing an arbitrary quantile <span class=math >$y$</span> as a rootfinding problem, and use Newton&#39;s method to write an algorithm for computing <span class=math >$x$</span>.</p> <p>&#40;Hint: Recall that <span class=math >$cdf^{\prime}(x) = pdf(x)$</span>, the probability distribution function.&#41;</p> <h3>Part 2</h3> <p>Use the types from Distributions.jl to write a function <code>my_quantile&#40;y,d&#41;</code> which uses multiple dispatch to compute the <span class=math >$y$</span>th quantile for any <code>UnivariateDistribution</code> <code>d</code> from Distributions.jl. Test your function on <code>Gamma&#40;5, 1&#41;</code>, <code>Normal&#40;0, 1&#41;</code>, and <code>Beta&#40;2, 4&#41;</code> against the <code>Distributions.quantile</code> function built into the library.</p> <p>&#40;Hint: Have a keyword argument for <span class=math >$x_0$</span>, and let its default be the mean or median of the distribution.&#41;</p> <h2>Problem 3: Bifurcating Data for Parallelism</h2> <p>In this problem we will write code for efficient generation of the bifurcation diagram of the logistic equation.</p> <h3>Part 1</h3> <p>The logistic equation is the dynamical system given by the update relation:</p> <p class=math >\[ x_{n+1} = rx_n (1-x_n) \]</p> <p>where <span class=math >$r$</span> is some parameter. Write a function which iterates the equation from <span class=math >$x_0 = 0.25$</span> enough times to be sufficiently close to its long-term behavior &#40;400 iterations&#41; and samples 150 points from the steady state attractor &#40;i.e. output iterations 401:550&#41; as a function of <span class=math >$r$</span>, and mutates some vector as a solution, i.e. <code>calc_attractor&#33;&#40;out,f,p,num_attract&#61;150;warmup&#61;400&#41;</code>.</p> <p>Test your function with <span class=math >$r = 2.9$</span>. Double check that your function computes the correct result by calculating the analytical steady state value.</p> <h3>Part 2</h3> <p>The bifurcation plot shows how a steady state changes as a parameter changes. Compute the long-term result of the logistic equation at the values of <code>r &#61; 2.9:0.001:4</code>, and plot the steady state values for each <span class=math >$r$</span> as an r x steady_attractor scatter plot. You should get a very bizarrely awesome picture, the bifurcation graph of the logistic equation.</p> <p><img src="https://upload.wikimedia.org/wikipedia/commons/7/7d/LogisticMap_BifurcationDiagram.png" alt="" /></p> <p>&#40;Hint: Generate a single matrix for the attractor values, and use <code>calc_attractor&#33;</code> on views of columns for calculating the output, or inline the <code>calc_attractor&#33;</code> computation directly onto the matrix, or even give <code>calc_attractor&#33;</code> an input for what column to modify.&#41;</p> <h3>Part 3</h3> <p>Multithread your bifurcation graph generator by performing different steady state calculations on different threads. Does your timing improve? Why? Be careful and check to make sure you have more than 1 thread&#33;</p> <h3>Part 4</h3> <p>Multiprocess your bifurcation graph generator first by using <code>pmap</code>, and then by using <code>@distributed</code>. Does your timing improve? Why? Be careful to add processes before doing the distributed call.</p> <p>&#40;Note: You may need to change your implementation around to be allocating differently in order for it to be compatible with multiprocessing&#33;&#41;</p> <h3>Part 5</h3> <p>Which method is the fastest? Why?</p> <div class=footer > <p> Published from <a href=hw1.jmd >hw1.jmd</a> using <a href="http://github.com/JunoLab/Weave.jl">Weave.jl</a> v0.10.12 on 2025-02-03. </p> </div>